import streamlit as st
import pandas as pd
from rapidfuzz import fuzz, process
import unidecode
import re
import os
import pickle
from collections import Counter

# --- 1. C·∫§U H√åNH CLASS MACHINE LEARNING (PHARMA BRAIN) ---
class PharmaBrain:
    def __init__(self):
        self.brand_memory = {}  # B·ªô nh·ªõ: T·ª´ kh√≥a -> T√™n H√£ng Chu·∫©n
        self.learned_status = False

    def _tokenize(self, text):
        """T√°ch chu·ªói th√†nh t·ª´ kh√≥a (tokens)"""
        if pd.isna(text): return []
        text = unidecode.unidecode(str(text).lower())
        return re.findall(r"\w+", text)

    def learn(self, history_df, input_col, brand_col):
        """H·ªçc t·ª´ file l·ªãch s·ª≠ c≈©"""
        brand_counter = {}
        count_learned = 0
        
        for _, row in history_df.iterrows():
            raw_text = row[input_col]
            true_brand = row[brand_col]
            if pd.isna(true_brand) or pd.isna(raw_text): continue
            
            tokens = self._tokenize(raw_text)
            for token in tokens:
                if len(token) < 2 or token.isdigit(): continue # B·ªè qua t·ª´ qu√° ng·∫Øn ho·∫∑c s·ªë
                
                if token not in brand_counter: brand_counter[token] = Counter()
                brand_counter[token][true_brand] += 1

        # Ch·ªâ nh·ªõ quy lu·∫≠t c√≥ ƒë·ªô tin c·∫≠y > 70%
        self.brand_memory = {}
        for token, counts in brand_counter.items():
            most_common_brand, count = counts.most_common(1)[0]
            total = sum(counts.values())
            confidence = count / total
            
            if total >= 2 and confidence > 0.7: # Quy t·∫Øc l·ªçc nhi·ªÖu
                self.brand_memory[token] = most_common_brand
                count_learned += 1
                
        self.learned_status = True
        return count_learned

    def predict_brand(self, raw_text):
        """D·ª± ƒëo√°n h√£ng t·ª´ t√™n thu·ªëc m·ªõi nh·∫≠p"""
        if not self.brand_memory: return None
        tokens = self._tokenize(raw_text)
        detected_brands = []
        for token in tokens:
            if token in self.brand_memory:
                detected_brands.append(self.brand_memory[token])
        
        if detected_brands:
            return Counter(detected_brands).most_common(1)[0][0]
        return None

    def save_model(self):
        with open("pharma_brain.pkl", "wb") as f: pickle.dump(self.brand_memory, f)

    def load_model(self):
        if os.path.exists("pharma_brain.pkl"):
            with open("pharma_brain.pkl", "rb") as f: self.brand_memory = pickle.load(f)
            self.learned_status = True
            return True
        return False

# --- 2. C√ÅC H√ÄM X·ª¨ L√ù TEXT & T√çNH ƒêI·ªÇM ---
def normalize_text(text):
    if pd.isna(text): return ""
    return unidecode.unidecode(str(text).lower()).strip()

def extract_numbers(text):
    if pd.isna(text): return set()
    return set(re.findall(r"\d+\.?\d*", str(text)))

def calculate_weighted_score(input_str, row_data, ml_predicted_brand=None):
    """
    T√≠nh ƒëi·ªÉm t·ªïng h·ª£p: Text + S·ªë + ML Bonus
    """
    norm_input = normalize_text(input_str)
    
    # 1. ƒêi·ªÉm T√™n (40%)
    score_name = fuzz.token_set_ratio(norm_input, row_data['norm_name'])
    
    # 2. ƒêi·ªÉm H√£ng (20%)
    score_brand = fuzz.partial_ratio(row_data['norm_brand'], norm_input)
    
    # 3. ƒêi·ªÉm Ho·∫°t ch·∫•t (20%)
    score_active = fuzz.token_set_ratio(row_data['norm_active'], norm_input)
    
    # 4. ƒêi·ªÉm H√†m l∆∞·ª£ng (10%) - Logic so kh·ªõp s·ªë
    input_nums = extract_numbers(input_str)
    row_nums = extract_numbers(row_data['ham_luong'])
    if not row_nums: score_strength = 50
    elif input_nums.intersection(row_nums): score_strength = 100
    else: score_strength = 0
    
    # 5. ƒêi·ªÉm D·∫°ng b√†o ch·∫ø (10%)
    score_form = fuzz.partial_ratio(row_data['norm_form'], norm_input)
    
    # --- T√çNH ƒêI·ªÇM C∆† B·∫¢N ---
    base_score = (score_name*0.4) + (score_brand*0.2) + (score_active*0.2) + (score_strength*0.1) + (score_form*0.1)
    
    # --- 6. ML BONUS (ƒêI·ªÇM TH∆Ø·ªûNG AI) ---
    ml_bonus = 0
    match_ml = "No"
    
    # N·∫øu AI d·ª± ƒëo√°n ƒë∆∞·ª£c h√£ng, v√† h√£ng ƒë√≥ tr√πng v·ªõi d√≤ng d·ªØ li·ªáu n√†y
    if ml_predicted_brand:
        # So s√°nh h√£ng d·ª± ƒëo√°n v·ªõi h√£ng trong data (fuzzy nh·∫π ƒë·ªÉ tr√°nh l·ªói ch√≠nh t·∫£)
        similarity = fuzz.token_set_ratio(normalize_text(ml_predicted_brand), row_data['norm_brand'])
        if similarity > 85: # N·∫øu kh·ªõp h√£ng > 85%
            ml_bonus = 15 # C·ªòNG 15 ƒêI·ªÇM TH∆Ø·ªûNG
            match_ml = "Yes"
            
    final_score = base_score + ml_bonus
    
    return {
        'total': min(final_score, 100), # Max l√† 100
        'detail': f"T√™n:{int(score_name)} | H√£ng:{int(score_brand)} | S·ªë:{int(score_strength)} | ML_Bonus:+{ml_bonus}",
        'ml_match': match_ml
    }

# --- 3. H√ÄM T√åM KI·∫æM CH√çNH ---
def search_product(input_text, db_df, brain_model, min_score=50, top_n=1):
    # B∆∞·ªõc 1: H·ªèi √Ω ki·∫øn AI tr∆∞·ªõc
    predicted_brand = brain_model.predict_brand(input_text)
    
    norm_input = normalize_text(input_text)
    
    # B∆∞·ªõc 2: L·ªçc s∆° b·ªô 50 ·ª©ng vi√™n b·∫±ng T√™n thu·ªëc
    candidates = process.extract(norm_input, db_df['norm_name'], limit=50, scorer=fuzz.token_set_ratio)
    candidate_indices = [x[2] for x in candidates if x[1] > 30] # L·∫•y n·∫øu gi·ªëng > 30%
    
    if not candidate_indices: return []

    subset = db_df.iloc[candidate_indices].copy()
    results = []
    
    # B∆∞·ªõc 3: Ch·∫•m ƒëi·ªÉm chi ti·∫øt t·ª´ng ·ª©ng vi√™n
    for idx, row in subset.iterrows():
        # Truy·ªÅn d·ª± ƒëo√°n c·ªßa AI v√†o h√†m ch·∫•m ƒëi·ªÉm
        scoring = calculate_weighted_score(input_text, row, ml_predicted_brand=predicted_brand)
        
        if scoring['total'] >= min_score:
            results.append({
                'M√£ VTMA': row['ma_vtma'],
                'T√™n VTMA': row['ten_thuoc'],
                'NSX': row['ten_cong_ty'],
                'AI D·ª± ƒêo√°n NSX': predicted_brand if predicted_brand else "-",
                'ƒêi·ªÉm T·ªïng': round(scoring['total'], 1),
                'Chi Ti·∫øt ƒêi·ªÉm': scoring['detail']
            })
            
    # S·∫Øp x·∫øp v√† c·∫Øt Top N
    results.sort(key=lambda x: x['ƒêi·ªÉm T·ªïng'], reverse=True)
    return results[:top_n]

# --- 4. GIAO DI·ªÜN STREAMLIT ---
st.set_page_config(page_title="PharmaMaster AI", layout="wide", page_icon="üíä")

# Kh·ªüi t·∫°o App State
if 'brain' not in st.session_state:
    st.session_state.brain = PharmaBrain()
    st.session_state.brain.load_model() # T·ª± ƒë·ªông load n·∫øu c√≥ file c≈©

if 'db_vtma' not in st.session_state:
    # --- MOCK DATA (D·ªÆ LI·ªÜU GI·∫¢ L·∫¨P ƒê·ªÇ CH·∫†Y NGAY) ---
    data = {
        'ma_vtma': ['V01', 'V02', 'V03', 'V04'],
        'ten_thuoc': ['Hapacol 650', 'Panadol Extra', 'Efferalgan', 'Augmentin 1g'],
        'ten_cong_ty': ['DHG Pharma', 'GSK', 'UPSA', 'GSK'],
        'hoat_chat': ['Paracetamol', 'Para, Cafein', 'Paracetamol', 'Amoxicillin'],
        'ham_luong': ['650mg', '500mg', '500mg', '1g'],
        'dang_bao_che': ['Vi√™n n√©n', 'Vi√™n n√©n', 'S·ªßi', 'B·ªôt']
    }
    df = pd.DataFrame(data)
    # Chu·∫©n h√≥a Data 1 l·∫ßn
    df['norm_name'] = df['ten_thuoc'].apply(normalize_text)
    df['norm_brand'] = df['ten_cong_ty'].apply(normalize_text)
    df['norm_active'] = df['hoat_chat'].apply(normalize_text)
    df['norm_form'] = df['dang_bao_che'].apply(normalize_text)
    df['norm_strength'] = df['ham_luong'].apply(normalize_text)
    st.session_state.db_vt
