import streamlit as st
import pandas as pd
from rapidfuzz import fuzz, process
import unidecode
import re
import os
import pickle
from collections import Counter

# =============================================================================
# 1. Cáº¤U HÃŒNH TRANG & CLASS MACHINE LEARNING (TRÃ TUá»† NHÃ‚N Táº O)
# =============================================================================
st.set_page_config(page_title="PharmaMaster: AI Matching", layout="wide", page_icon="ğŸ’Š")

class PharmaBrain:
    """
    Bá»™ nÃ£o AI: CÃ³ kháº£ nÄƒng há»c tá»« lá»‹ch sá»­ mapping cÅ© Ä‘á»ƒ nháº­n diá»‡n NhÃ  Sáº£n Xuáº¥t.
    """
    def __init__(self):
        self.brand_memory = {}  # Bá»™ nhá»›: { 'tá»«_khÃ³a': 'TÃªn_HÃ£ng_Chuáº©n' }
        self.learned_status = False

    def _tokenize(self, text):
        """TÃ¡ch chuá»—i thÃ nh cÃ¡c tá»« khÃ³a nhá» (tokens)"""
        if pd.isna(text): return []
        text = unidecode.unidecode(str(text).lower()) # Chuyá»ƒn vá» tiáº¿ng Viá»‡t khÃ´ng dáº¥u
        return re.findall(r"\w+", text) # TÃ¡ch tá»«

    def learn(self, history_df, input_col, brand_col):
        """Há»c quy luáº­t tá»« file Excel lá»‹ch sá»­"""
        brand_counter = {}
        count_learned = 0
        
        # Duyá»‡t qua tá»«ng dÃ²ng lá»‹ch sá»­
        for _, row in history_df.iterrows():
            raw_text = row[input_col]
            true_brand = row[brand_col]
            
            if pd.isna(true_brand) or pd.isna(raw_text): continue
            
            tokens = self._tokenize(raw_text)
            for token in tokens:
                # Bá» qua tá»« quÃ¡ ngáº¯n (<2 kÃ½ tá»±) hoáº·c toÃ n sá»‘
                if len(token) < 2 or token.isdigit(): continue
                
                if token not in brand_counter: brand_counter[token] = Counter()
                brand_counter[token][true_brand] += 1

        # Lá»ŒC NHIá»„U: Chá»‰ nhá»› quy luáº­t nÃ o cÃ³ Ä‘á»™ tin cáº­y > 70%
        self.brand_memory = {}
        for token, counts in brand_counter.items():
            most_common_brand, count = counts.most_common(1)[0]
            total = sum(counts.values())
            confidence = count / total
            
            # Quy táº¯c: Xuáº¥t hiá»‡n Ã­t nháº¥t 2 láº§n vÃ  Ä‘á»™ chÃ­nh xÃ¡c > 70%
            if total >= 2 and confidence > 0.7: 
                self.brand_memory[token] = most_common_brand
                count_learned += 1
                
        self.learned_status = True
        return count_learned

    def predict_brand(self, raw_text):
        """Dá»± Ä‘oÃ¡n hÃ£ng sáº£n xuáº¥t dá»±a trÃªn tÃªn thuá»‘c má»›i nháº­p"""
        if not self.brand_memory: return None
        
        tokens = self._tokenize(raw_text)
        detected_brands = []
        
        for token in tokens:
            if token in self.brand_memory:
                detected_brands.append(self.brand_memory[token])
        
        # Tráº£ vá» hÃ£ng xuáº¥t hiá»‡n nhiá»u nháº¥t trong cÃ¢u
        if detected_brands:
            return Counter(detected_brands).most_common(1)[0][0]
        return None

    def save_model(self, path="pharma_brain.pkl"):
        """LÆ°u bá»™ nhá»› ra file Ä‘á»ƒ dÃ¹ng láº§n sau"""
        with open(path, "wb") as f: pickle.dump(self.brand_memory, f)

    def load_model(self, path="pharma_brain.pkl"):
        """Náº¡p bá»™ nhá»› tá»« file Ä‘Ã£ lÆ°u"""
        if os.path.exists(path):
            with open(path, "rb") as f: self.brand_memory = pickle.load(f)
            self.learned_status = True
            return True
        return False

# =============================================================================
# 2. CÃC HÃ€M Xá»¬ LÃ Dá»® LIá»†U & TÃNH ÄIá»‚M (CORE LOGIC)
# =============================================================================

def normalize_text(text):
    """Chuáº©n hÃ³a text: Chá»¯ thÆ°á»ng, bá» dáº¥u, cáº¯t khoáº£ng tráº¯ng"""
    if pd.isna(text): return ""
    return unidecode.unidecode(str(text).lower()).strip()

def extract_numbers(text):
    """Láº¥y táº­p há»£p cÃ¡c con sá»‘ tá»« chuá»—i (VD: '500mg' -> {500})"""
    if pd.isna(text): return set()
    # Regex láº¥y sá»‘ thá»±c (integer hoáº·c float)
    return set(re.findall(r"\d+\.?\d*", str(text)))

@st.cache_data
def load_master_data():
    """PhiÃªn báº£n Debug: Tá»± Ä‘á»™ng dÃ² dáº¥u phÃ¢n cÃ¡ch vÃ  in tÃªn cá»™t ra mÃ n hÃ¬nh"""
    file_path = "data/vtma_standard.csv"
    
    if not os.path.exists(file_path):
        st.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y file '{file_path}'")
        return None

    try:
        # 1. Äá»c file thÃ´ng minh: Tá»± Ä‘á»™ng nháº­n diá»‡n dáº¥u pháº©y (,) hay Tab (\t)
        # engine='python' giÃºp tá»± dÃ² separator
        df = pd.read_csv(file_path, sep=None, engine='python', encoding='utf-8')
    except:
        try:
            # Náº¿u lá»—i encoding, thá»­ láº¡i vá»›i latin1
            df = pd.read_csv(file_path, sep=None, engine='python', encoding='latin1')
        except Exception as e:
            st.error(f"âŒ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c file. Lá»—i: {e}")
            return None

    # 2. Chuáº©n hÃ³a tÃªn cá»™t: XÃ³a khoáº£ng tráº¯ng thá»«a, vá» chá»¯ thÆ°á»ng
    # VÃ­ dá»¥: " VTMA Code " -> "vtma code"
    df.columns = df.columns.str.strip().str.lower()
    
    # -----------------------------------------------------------
    # ğŸ” DEBUG: IN RA TÃŠN Cá»˜T THá»°C Táº¾ Äá»‚ Báº N KIá»‚M TRA
    # -----------------------------------------------------------
    # Náº¿u code cháº¡y ok thÃ¬ dÃ²ng nÃ y sáº½ áº©n Ä‘i, náº¿u lá»—i nÃ³ giÃºp báº¡n biáº¿t file cÃ³ gÃ¬
    # st.write("ğŸ” Debug - CÃ¡c cá»™t tÃ¬m tháº¥y trong file:", df.columns.tolist())
    
    # 3. MAPPING LINH HOáº T HÆ N
    # Táº¡o danh sÃ¡ch cÃ¡c tÃªn cá»™t thÆ°á»ng gáº·p Ä‘á»ƒ map vá» chuáº©n
    mapping_dict = {
        'ma_vtma': ['vtma code', 'ma thuoc', 'ma_vtma', 'vtma_code', 'code'],
        'ten_thuoc': ['product', 'ten thuoc', 'ten_thuoc', 'name', 'ten'],
        'hoat_chat': ['molecule', 'hoat chat', 'hoat_chat', 'active ingredient'],
        'ten_cong_ty': ['manufacturer', 'corporation', 'ten cong ty', 'nha san xuat', 'hang sx'],
        'ham_luong': ['galenic', 'ham luong', 'nong do', 'strength'],
        'dang_bao_che': ['unit_measure', 'dang bao che', 'dosage form', 'form'],
        'sku_full': ['sku', 'sku name', 'ten day du']
    }

    # Thá»±c hiá»‡n Ä‘á»•i tÃªn dá»±a trÃªn tá»« Ä‘iá»ƒn trÃªn
    final_rename_map = {}
    current_cols = df.columns.tolist()
    
    for standard_col, aliases in mapping_dict.items():
        found = False
        for alias in aliases:
            if alias in current_cols:
                final_rename_map[alias] = standard_col
                found = True
                break # ÄÃ£ tÃ¬m tháº¥y thÃ¬ dá»«ng, sang cá»™t tiáº¿p theo
    
    if final_rename_map:
        df.rename(columns=final_rename_map, inplace=True)

    # 4. KIá»‚M TRA Láº I SAU KHI MAP
    required_cols = ['ma_vtma', 'ten_thuoc', 'ten_cong_ty', 'hoat_chat', 'ham_luong', 'dang_bao_che']
    missing = [c for c in required_cols if c not in df.columns]
    
    if missing:
        st.error("âš ï¸ Lá»–I Cáº¤U TRÃšC FILE CSV")
        st.error(f"Pháº§n má»m cáº§n cá»™t: **{required_cols}**")
        st.warning(f"NhÆ°ng trong file cá»§a báº¡n sau khi Ä‘á»c chá»‰ cÃ³: {df.columns.tolist()}")
        st.info("ğŸ’¡ Gá»£i Ã½: HÃ£y má»Ÿ file CSV báº±ng Excel, sá»­a dÃ²ng Ä‘áº§u tiÃªn (Header) thÃ nh: vtma code, product, manufacturer, molecule, galenic, unit_measure")
        st.stop()

    # 5. Xá»­ lÃ½ dá»¯ liá»‡u text (Logic cÅ©)
    df['norm_name'] = df['ten_thuoc'].apply(normalize_text)
    df['norm_brand'] = df['ten_cong_ty'].apply(normalize_text)
    df['norm_active'] = df['hoat_chat'].apply(normalize_text)
    df['norm_form'] = df['dang_bao_che'].apply(normalize_text)
    df['norm_strength'] = df['ham_luong'].apply(normalize_text)

    # Cá»™t hiá»ƒn thá»‹
    if 'sku_full' in df.columns:
        df['display_name'] = df['sku_full']
    else:
        df['display_name'] = df['ten_thuoc'] + " " + df['ham_luong']

    return df
def calculate_weighted_score(input_str, row_data, ml_predicted_brand=None):
    """
    TÃ­nh Ä‘iá»ƒm khá»›p (0-100) dá»±a trÃªn 5 tiÃªu chÃ­ + Äiá»ƒm thÆ°á»Ÿng AI
    """
    norm_input = normalize_text(input_str)
    
    # 1. TÃªn thuá»‘c (40%)
    score_name = fuzz.token_set_ratio(norm_input, row_data['norm_name'])
    
    # 2. ThÆ°Æ¡ng hiá»‡u/HÃ£ng (20%)
    score_brand = fuzz.partial_ratio(row_data['norm_brand'], norm_input)
    
    # 3. Hoáº¡t cháº¥t (20%)
    score_active = fuzz.token_set_ratio(row_data['norm_active'], norm_input)
    
    # 4. HÃ m lÆ°á»£ng (10%) - Logic Ä‘áº·c biá»‡t: Khá»›p sá»‘
    input_nums = extract_numbers(input_str)
    row_nums = extract_numbers(row_data['ham_luong'])
    if not row_nums: score_strength = 50 # KhÃ´ng cÃ³ sá»‘ liá»‡u thÃ¬ cho Ä‘iá»ƒm trung bÃ¬nh
    elif input_nums.intersection(row_nums): score_strength = 100 # CÃ³ sá»‘ trÃ¹ng nhau
    else: score_strength = 0 # CÃ³ sá»‘ nhÆ°ng khÃ¡c nhau (lá»‡ch hÃ m lÆ°á»£ng)
    
    # 5. Dáº¡ng bÃ o cháº¿ (10%)
    score_form = fuzz.partial_ratio(row_data['norm_form'], norm_input)
    
    # --- Tá»”NG ÄIá»‚M CÆ  Báº¢N ---
    base_score = (score_name*0.4) + (score_brand*0.2) + (score_active*0.2) + (score_strength*0.1) + (score_form*0.1)
    
    # --- 6. ÄIá»‚M THÆ¯á»NG AI (TRUST BONUS) ---
    ml_bonus = 0
    match_ml = "No"
    
    if ml_predicted_brand:
        # Náº¿u AI Ä‘oÃ¡n ra hÃ£ng, vÃ  hÃ£ng Ä‘Ã³ khá»›p vá»›i dá»¯ liá»‡u dÃ²ng nÃ y (>85%)
        similarity = fuzz.token_set_ratio(normalize_text(ml_predicted_brand), row_data['norm_brand'])
        if similarity > 85:
            ml_bonus = 15 # Cá»™ng 15 Ä‘iá»ƒm
            match_ml = "Yes"
            
    final_score = base_score + ml_bonus
    
    return {
        'total': min(final_score, 100), # Max lÃ  100
        'detail': f"TÃªn:{int(score_name)} | HÃ£ng:{int(score_brand)} | Sá»‘:{int(score_strength)} | ML:+{ml_bonus}",
        'ml_match': match_ml
    }

def search_product(input_text, db_df, brain_model, min_score=50, top_n=1):
    """HÃ m tÃ¬m kiáº¿m chÃ­nh"""
    # B1: AI dá»± Ä‘oÃ¡n hÃ£ng
    predicted_brand = brain_model.predict_brand(input_text)
    
    # B2: Lá»c thÃ´ (Heuristic) - Láº¥y Top 50 tÃªn giá»‘ng nháº¥t Ä‘á»ƒ tÃ­nh toÃ¡n cho nhanh
    norm_input = normalize_text(input_text)
    candidates = process.extract(norm_input, db_df['norm_name'], limit=50, scorer=fuzz.token_set_ratio)
    candidate_indices = [x[2] for x in candidates if x[1] > 30] # Chá»‰ láº¥y náº¿u giá»‘ng > 30%
    
    if not candidate_indices: return []

    subset = db_df.iloc[candidate_indices].copy()
    results = []
    
    # B3: Cháº¥m Ä‘iá»ƒm chi tiáº¿t
    for idx, row in subset.iterrows():
        scoring = calculate_weighted_score(input_text, row, ml_predicted_brand=predicted_brand)
        
        if scoring['total'] >= min_score:
            results.append({
                'MÃ£ VTMA': row['ma_vtma'],
                'TÃªn Thuá»‘c (SKU)': row['display_name'],
                'NSX Chuáº©n': row['ten_cong_ty'],
                'AI Dá»± ÄoÃ¡n': predicted_brand if predicted_brand else "-",
                'Äiá»ƒm': round(scoring['total'], 1),
                'Chi Tiáº¿t': scoring['detail']
            })
            
    # B4: Sáº¯p xáº¿p & Cáº¯t Top N
    results.sort(key=lambda x: x['Äiá»ƒm'], reverse=True)
    return results[:top_n]

# =============================================================================
# 3. GIAO DIá»†N NGÆ¯á»œI DÃ™NG (STREAMLIT UI)
# =============================================================================

# --- A. KHá»I Táº O STATE ---
if 'brain' not in st.session_state:
    st.session_state.brain = PharmaBrain()
    st.session_state.brain.load_model() # Load bá»™ nÃ£o cÅ© náº¿u cÃ³

if 'db_vtma' not in st.session_state:
    with st.spinner("â³ Äang táº£i Master Data..."):
        df_loaded = load_master_data()
        if df_loaded is not None:
            st.session_state.db_vtma = df_loaded
        else:
            st.stop() # Dá»«ng App náº¿u khÃ´ng load Ä‘Æ°á»£c Data

# --- B. SIDEBAR ---
with st.sidebar:
    st.header("âš™ï¸ Cáº¥u hÃ¬nh Mapping")
    min_score = st.slider("NgÆ°á»¡ng Ä‘iá»ƒm tá»‘i thiá»ƒu", 0, 100, 60, help="DÆ°á»›i Ä‘iá»ƒm nÃ y coi nhÆ° khÃ´ng tÃ¬m tháº¥y")
    top_n = st.number_input("Sá»‘ lÆ°á»£ng káº¿t quáº£ (Top N)", 1, 10, 1)
    st.divider()
    st.write(f"ğŸ“Š Database: **{len(st.session_state.db_vtma)}** SKU")
    st.write(f"ğŸ§  Tráº¡ng thÃ¡i AI: **{'ÄÃ£ há»c' if st.session_state.brain.learned_status else 'ChÆ°a há»c'}**")

st.title("ğŸ’Š PharmaMaster: Há»‡ Thá»‘ng Mapping Thuá»‘c ThÃ´ng Minh")

# --- C. MAIN TABS ---
tab1, tab2 = st.tabs(["ğŸš€ Cháº¡y Mapping (Run)", "ğŸ§  Dáº¡y AI (Train)"])

# TAB 1: CHáº Y MAPPING
with tab1:
    st.subheader("1. Mapping Dá»¯ Liá»‡u Má»›i")
    
    # Input Text (Test nhanh)
    col_search, col_btn = st.columns([3, 1])
    with col_search:
        test_txt = st.text_input("Nháº­p tÃªn thuá»‘c Ä‘á»ƒ test nhanh:", placeholder="VÃ­ dá»¥: Hapacol 650 dhg")
    
    if test_txt:
        res = search_product(test_txt, st.session_state.db_vtma, st.session_state.brain, min_score, top_n)
        if res:
            df_res = pd.DataFrame(res)
            # Highlight dÃ²ng Ä‘Æ°á»£c cá»™ng Ä‘iá»ƒm AI
            st.dataframe(df_res.style.apply(lambda x: ['background-color: #d1e7dd' if "ML:+" in str(x['Chi Tiáº¿t']) else '' for i in x], axis=1), use_container_width=True)
        else:
            st.warning("KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ phÃ¹ há»£p.")

    st.divider()
    
    # Upload File (Cháº¡y hÃ ng loáº¡t)
    st.write("ğŸ“‚ **Upload file Excel danh sÃ¡ch cáº§n Map:**")
    uploaded_file = st.file_uploader("Chá»n file (.xlsx, .csv)", type=['xlsx', 'csv'])
    
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'): df_in = pd.read_csv(uploaded_file)
            else: df_in = pd.read_excel(uploaded_file)
            
            st.write(f"ÄÃ£ nháº­n {len(df_in)} dÃ²ng dá»¯ liá»‡u.")
            col_target = st.selectbox("Chá»n cá»™t chá»©a tÃªn thuá»‘c:", df_in.columns)
            
            if st.button("ğŸš€ Báº®T Äáº¦U MAPPING HÃ€NG LOáº T"):
                results_batch = []
                progress_bar = st.progress(0)
                
                for i, row in df_in.iterrows():
                    input_val = str(row[col_target])
                    matches = search_product(input_val, st.session_state.db_vtma, st.session_state.brain, min_score, 1)
                    
                    if matches:
                        match = matches[0] # Láº¥y Top 1
                        results_batch.append({
                            'Input_Goc': input_val,
                            'Ma_VTMA': match['MÃ£ VTMA'],
                            'Ten_VTMA': match['TÃªn Thuá»‘c (SKU)'],
                            'Diem': match['Äiá»ƒm'],
                            'Ghi_Chu': match['Chi Tiáº¿t']
                        })
                    else:
                        results_batch.append({'Input_Goc': input_val, 'Ma_VTMA': '', 'Ten_VTMA': 'KhÃ´ng tÃ¬m tháº¥y', 'Diem': 0})
                    
                    progress_bar.progress((i + 1) / len(df_in))
                
                df_out = pd.DataFrame(results_batch)
                st.success("âœ… HoÃ n táº¥t!")
                st.dataframe(df_out)
                
                # Download
                csv = df_out.to_csv(index=False).encode('utf-8')
                st.download_button("ğŸ“¥ Táº£i káº¿t quáº£ (CSV)", csv, "ket_qua_map.csv", "text/csv")
                
        except Exception as e:
            st.error(f"Lá»—i khi Ä‘á»c file: {e}")

# TAB 2: Dáº Y MÃY Há»ŒC
with tab2:
    st.subheader("2. Huáº¥n luyá»‡n AI (Supervised Learning)")
    st.info("Upload file lá»‹ch sá»­ Ä‘Ã£ map Ä‘Ãºng Ä‘á»ƒ mÃ¡y há»c cÃ¡ch nháº­n diá»‡n NhÃ  Sáº£n Xuáº¥t tá»« tÃªn viáº¿t táº¯t.")
    
    uploaded_hist = st.file_uploader("Chá»n file lá»‹ch sá»­ (.xlsx)", key="hist")
    
    if uploaded_hist:
        df_hist = pd.read_excel(uploaded_hist)
        st.dataframe(df_hist.head(3))
        
        c1, c2 = st.columns(2)
        col_in = c1.selectbox("Cá»™t TÃªn Gá»‘c (Input)", df_hist.columns)
        col_out = c2.selectbox("Cá»™t HÃ£ng Chuáº©n (Target)", df_hist.columns)
        
        if st.button("ğŸ“ Báº®T Äáº¦U Dáº Y MÃY"):
            with st.spinner("Äang phÃ¢n tÃ­ch dá»¯ liá»‡u..."):
                n_learned = st.session_state.brain.learn(df_hist, col_in, col_out)
                st.session_state.brain.save_model()
            
            st.success(f"ğŸ‰ ÄÃ£ há»c xong! MÃ¡y Ä‘Ã£ ghi nhá»› {n_learned} quy luáº­t nháº­n diá»‡n hÃ£ng má»›i.")
            st.json(st.session_state.brain.brand_memory)
