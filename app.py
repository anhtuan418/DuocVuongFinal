import streamlit as st
import pandas as pd
import google.generativeai as genai
from rapidfuzz import fuzz, process
import unidecode
import json
import os
import re
import time
from datetime import datetime

# --- C·∫§U H√åNH ---
st.set_page_config(page_title="PharmaMatch: Batch Speed", layout="wide")

def normalize_text(text):
    if pd.isna(text): return ""
    return unidecode.unidecode(str(text).lower()).strip()

def extract_numbers(text):
    """L·∫•y t·∫≠p h·ª£p s·ªë ƒë·ªÉ so s√°nh ch√≠nh x√°c."""
    if pd.isna(text): return set()
    nums = re.findall(r"\d+\.?\d*", str(text))
    return set(nums)

# --- LOAD DATA ---
@st.cache_data
def load_vtma_data():
    try:
        df = pd.read_csv("data/vtma_standard.csv")
        df['norm_name'] = df['ten_thuoc'].apply(normalize_text)
        df['norm_strength'] = df['ham_luong'].apply(normalize_text)
        df['norm_ingre'] = df['hoat_chat'].apply(normalize_text)
        df['norm_manu'] = df['ten_cong_ty'].apply(normalize_text)
        return df
    except:
        return pd.DataFrame()

# --- AI BATCH PROCESSING (G·ªòP NHI·ªÄU D√íNG) ---
def ai_process_batch(product_list, api_key):
    """G·ª≠i 1 danh s√°ch s·∫£n ph·∫©m l√™n AI c√πng l√∫c"""
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        # T·∫°o prompt danh s√°ch
        items_str = "\n".join([f"- ID_{i}: {p}" for i, p in enumerate(product_list)])
        
        prompt = f"""
        Danh s√°ch thu·ªëc c·∫ßn tr√≠ch xu·∫•t th√¥ng tin:
        {items_str}
        
        Y√™u c·∫ßu tr·∫£ v·ªÅ JSON d·∫°ng List of Objects (Tuy·ªát ƒë·ªëi kh√¥ng Markdown), m·ªói object g·ªìm:
        - "id": "ID_..." (gi·ªØ nguy√™n ID t∆∞∆°ng ·ª©ng)
        - "brand_name": T√™n bi·ªát d∆∞·ª£c
        - "strength": H√†m l∆∞·ª£ng s·ªë (VD: 500mg, 10mg). Null n·∫øu kh√¥ng c√≥.
        - "active_ingredient": Ho·∫°t ch·∫•t.
        - "manufacturer": T√™n h√£ng.
        """
        
        response = model.generate_content(prompt)
        text = response.text.replace('```json', '').replace('```', '').strip()
        
        # Parse JSON
        data = json.loads(text)
        
        # Chuy·ªÉn v·ªÅ dict ƒë·ªÉ d·ªÖ map l·∫°i: {'ID_0': {...}, 'ID_1': {...}}
        result_dict = {item['id']: item for item in data}
        return result_dict
        
    except Exception as e:
        # N·∫øu l·ªói c·∫£ batch, tr·∫£ v·ªÅ r·ªóng ƒë·ªÉ x·ª≠ l√Ω sau (ho·∫∑c in l·ªói ra console)
        print(f"Batch Error: {e}")
        return {}

# --- LOGIC MATCHING (GI·ªÆ NGUY√äN ƒê·ªÇ ƒê·∫¢M B·∫¢O CH√çNH X√ÅC) ---
def hierarchical_match(input_data, vtma_df):
    if not input_data: return None, 0, "AI L·ªói"
    
    input_brand = normalize_text(input_data.get('brand_name', ''))
    input_strength = normalize_text(input_data.get('strength', ''))
    input_ingre = normalize_text(input_data.get('active_ingredient', ''))
    
    # 1. L·ªçc theo T√™n (Brand Name)
    candidates = process.extract(
        input_brand, 
        vtma_df['norm_name'], 
        limit=30, 
        scorer=fuzz.token_set_ratio
    )
    
    candidate_indices = [x[2] for x in candidates if x[1] >= 50]
    if not candidate_indices: return None, 0, "Kh√¥ng t√¨m th·∫•y t√™n"

    subset_df = vtma_df.iloc[candidate_indices].copy()
    
    # 2. Re-rank
    results = []
    input_nums = extract_numbers(input_strength)
    
    for idx, row in subset_df.iterrows():
        name_score = fuzz.token_set_ratio(input_brand, row['norm_name']) * 0.4
        
        # Logic H√†m L∆∞·ª£ng Nghi√™m Ng·∫∑t
        str_score = 0
        row_nums = extract_numbers(row['norm_strength'])
        
        if not input_nums: 
            str_score = fuzz.ratio(input_strength, row['norm_strength']) * 0.4
        else:
            # N·∫øu Input c√≥ s·ªë, b·∫Øt bu·ªôc VTMA ph·∫£i ch·ª©a t·∫≠p s·ªë ƒë√≥
            if input_nums.issubset(row_nums) or row_nums.issubset(input_nums):
                str_score = 40 
            else:
                str_score = 0 # Ph·∫°t n·∫∑ng
        
        ing_score = fuzz.token_sort_ratio(input_ingre, row['norm_ingre']) * 0.2
        
        final_score = name_score + str_score + ing_score
        results.append({'row': row, 'score': final_score})
    
    results.sort(key=lambda x: x['score'], reverse=True)
    if results:
        best = results[0]
        return best['row'], best['score'], "OK"
    return None, 0, "Low Score"

# --- GIAO DI·ªÜN ---
st.title("üöÄ PharmaMatch: T·ªëc ƒê·ªô Cao (Batch Processing)")
st.info("Ch·∫ø ƒë·ªô G·ªôp ƒê∆°n: X·ª≠ l√Ω 10 s·∫£n ph·∫©m c√πng l√∫c gi√∫p tƒÉng t·ªëc ƒë·ªô g·∫•p 5 l·∫ßn.")

with st.sidebar:
    api_key = st.text_input("Gemini API Key", type="password")
    if not api_key and "GENAI_API_KEY" in st.secrets:
        api_key = st.secrets["GENAI_API_KEY"]
    
    batch_size = st.slider("K√≠ch th∆∞·ªõc g√≥i (Batch Size)", 5, 20, 10, help="S·ªë l∆∞·ª£ng SP g·ª≠i ƒëi 1 l·∫ßn. M·∫°ng kho·∫ª th√¨ ƒë·ªÉ cao.")

vtma_df = load_vtma_data()
if vtma_df.empty: st.stop()

uploaded = st.file_uploader("Upload File D∆∞·ª£c V∆∞∆°ng", type=['xlsx', 'csv'])

if uploaded and st.button("üöÄ CH·∫†Y BATCH MAPPING"):
    if not api_key: st.stop()
    
    if uploaded.name.endswith('.csv'): df_in = pd.read_csv(uploaded)
    else: df_in = pd.read_excel(uploaded)
    
    col_name = df_in.columns[0]
    results = []
    
    # Chia d·ªØ li·ªáu th√†nh c√°c batch (g√≥i nh·ªè)
    input_data = df_in[col_name].astype(str).tolist()
    total_items = len(input_data)
    
    progress_bar = st.progress(0, text="ƒêang kh·ªüi ƒë·ªông...")
    
    # V√≤ng l·∫∑p x·ª≠ l√Ω t·ª´ng g√≥i
    for i in range(0, total_items, batch_size):
        batch_items = input_data[i : i + batch_size] # L·∫•y danh s√°ch 10 sp
        
        # 1. G·ªçi AI cho c·∫£ g√≥i
        try:
            ai_results_dict = ai_process_batch(batch_items, api_key)
        except:
            ai_results_dict = {} # N·∫øu l·ªói th√¨ b·ªè qua batch n√†y (ho·∫∑c retry n·∫øu mu·ªën ph·ª©c t·∫°p h∆°n)
        
        # 2. X·ª≠ l√Ω map cho t·ª´ng sp trong g√≥i
        for idx, item_name in enumerate(batch_items):
            item_id = f"ID_{idx}"
            ai_info = ai_results_dict.get(item_id, {})
            
            # Map v·ªõi VTMA
            match_row, score, note = hierarchical_match(ai_info, vtma_df)
            
            # Ghi k·∫øt qu·∫£
            res = {
                'DV_Input': item_name,
                'AI_Data': f"{ai_info.get('brand_name')} {ai_info.get('strength')}",
                'VTMA_Code': '', 'VTMA_Name': '', 'VTMA_HamLuong': '',
                'Score': score, 'Danh_Gia': 'Th·∫•p'
            }
            
            if match_row is not None:
                res.update({
                    'VTMA
