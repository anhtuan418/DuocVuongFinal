import streamlit as st
import pandas as pd
from rapidfuzz import fuzz, process
import unidecode
import re
import os
import pickle
from collections import Counter
import google.generativeai as genai
import time

# =============================================================================
# 1. C·∫§U H√åNH TRANG
# =============================================================================
st.set_page_config(page_title="PharmaMaster AI", layout="wide", page_icon="üß¨")

# =============================================================================
# 2. CLASS GEMINI AI (AN TO√ÄN - FAILSAFE)
# =============================================================================
class GeminiAgent:
    def __init__(self, api_key, model_pref='gemini-1.5-flash'):
        self.is_ready = False
        self.current_model = "None"
        
        if api_key:
            try:
                genai.configure(api_key=api_key)
                
                # --- C∆† CH·∫æ T·ª∞ ƒê·ªòNG CH·ªåN MODEL ---
                # Danh s√°ch ∆∞u ti√™n: Model kh√°ch ch·ªçn -> 1.5 Flash -> Pro 1.0 (C≈© nh∆∞ng b·ªÅn)
                available_models = [m.name for m in genai.list_models()]
                
                # Chu·∫©n h√≥a t√™n model ƒë·∫ßu v√†o (th√™m 'models/' n·∫øu thi·∫øu)
                target_model = model_pref if model_pref.startswith('models/') else f'models/{model_pref}'
                
                if target_model in available_models:
                    self.model = genai.GenerativeModel(model_pref)
                    self.current_model = model_pref
                elif 'models/gemini-1.5-flash' in available_models:
                    self.model = genai.GenerativeModel('gemini-1.5-flash')
                    self.current_model = 'gemini-1.5-flash'
                else:
                    # Fallback cu·ªëi c√πng: gemini-pro (b·∫£n 1.0 ·ªïn ƒë·ªãnh nh·∫•t)
                    self.model = genai.GenerativeModel('gemini-pro')
                    self.current_model = 'gemini-pro (Legacy)'
                
                self.is_ready = True
            except Exception as e:
                st.error(f"L·ªói k·∫øt n·ªëi AI: {e}")
                self.is_ready = False
        else:
            self.is_ready = False

    def smart_match(self, input_drug, candidates_df):
        if not self.is_ready: return "‚ö†Ô∏è L·ªói: Ch∆∞a nh·∫≠p API Key ho·∫∑c Key sai"

        candidates_str = ""
        for idx, row in candidates_df.iterrows():
            candidates_str += f"- ID: {row['ma_vtma']} | T√™n: {row['ten_thuoc']} | HL: {row['ham_luong']} | NSX: {row['ten_cong_ty']}\n"

        prompt = f"""
        B·∫°n l√† D∆∞·ª£c sƒ©. T√¨m m√£ thu·ªëc chu·∫©n (ID) cho s·∫£n ph·∫©m ƒë·∫ßu v√†o.
        
        INPUT: "{input_drug}"
        
        DATABASE (·ª®ng vi√™n):
        {candidates_str}
        
        Y√äU C·∫¶U:
        1. So s√°nh Input v·ªõi ·ª©ng vi√™n (Ho·∫°t ch·∫•t, H√†m l∆∞·ª£ng, H√£ng).
        2. Ch·ªçn 1 ID kh·ªõp nh·∫•t. 
        
        TR·∫¢ L·ªúI 1 D√íNG DUY NH·∫§T:
        ID_CHON | ƒê·ªò_TIN_C·∫¨Y (Cao/V·ª´a/Th·∫•p) | L√ù DO NG·∫ÆN
        V√≠ d·ª•: VTMA_001 | Cao | Kh·ªõp ho√†n to√†n t√™n v√† h√£ng
        N·∫øu kh√¥ng kh·ªõp >70%, tr·∫£ v·ªÅ: "NONE | - | Kh√¥ng t√¨m th·∫•y"
        """
        
        try:
            response = self.model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            return f"AI Error: {str(e)}"

# =============================================================================
# 3. CLASS MACHINE LEARNING (PHARMA BRAIN)
# =============================================================================
class PharmaBrain:
    def __init__(self):
        self.brand_memory = {} 
        self.learned_status = False

    def _tokenize(self, text):
        if pd.isna(text): return []
        text = unidecode.unidecode(str(text).lower())
        return re.findall(r"\w+", text)

    def learn(self, history_df, input_col, brand_col):
        brand_counter = {}
        count_learned = 0
        for _, row in history_df.iterrows():
            raw_text = row[input_col]
            true_brand = row[brand_col]
            if pd.isna(true_brand) or pd.isna(raw_text): continue
            tokens = self._tokenize(raw_text)
            for token in tokens:
                if len(token) < 2 or token.isdigit(): continue
                if token not in brand_counter: brand_counter[token] = Counter()
                brand_counter[token][true_brand] += 1

        self.brand_memory = {}
        for token, counts in brand_counter.items():
            most_common_brand, count = counts.most_common(1)[0]
            total = sum(counts.values())
            if total >= 2 and (count / total) > 0.7: 
                self.brand_memory[token] = most_common_brand
                count_learned += 1
        self.learned_status = True
        return count_learned

    def predict_brand(self, raw_text):
        if not self.brand_memory: return None
        tokens = self._tokenize(raw_text)
        detected_brands = []
        for token in tokens:
            if token in self.brand_memory:
                detected_brands.append(self.brand_memory[token])
        if detected_brands:
            return Counter(detected_brands).most_common(1)[0][0]
        return None

    def save_model(self):
        with open("pharma_brain.pkl", "wb") as f: pickle.dump(self.brand_memory, f)

    def load_model(self):
        if os.path.exists("pharma_brain.pkl"):
            with open("pharma_brain.pkl", "rb") as f: self.brand_memory = pickle.load(f)
            self.learned_status = True
            return True
        return False

# =============================================================================
# 4. X·ª¨ L√ù D·ªÆ LI·ªÜU & LOAD FILE
# =============================================================================
def normalize_text(text):
    if pd.isna(text): return ""
    return unidecode.unidecode(str(text).lower()).strip()

def extract_numbers(text):
    if pd.isna(text): return set()
    clean_text = str(text).replace('/', ' ').replace('-', ' ').replace('+', ' ')
    nums = re.findall(r"\d+\.?\d*", clean_text)
    return {float(n) for n in nums}

@st.cache_data
def load_master_data():
    file_path = "data/vtma_standard.csv"
    if not os.path.exists(file_path):
        st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file data t·∫°i: {file_path}")
        return None

    try:
        df = pd.read_csv(file_path, sep=None, engine='python', encoding='utf-8-sig')
    except:
        try:
            df = pd.read_csv(file_path, sep=None, engine='python', encoding='latin1')
        except Exception as e:
            st.error(f"‚ùå L·ªói ƒë·ªçc file: {e}")
            return None

    df.columns = df.columns.str.strip().str.lower().str.replace('\ufeff', '').str.replace('√Ø¬ª¬ø', '')
    
    mapping_dict = {
        'ma_vtma': ['ma_thuoc', 'vtma code', 'code'],
        'ten_thuoc': ['ten_thuoc', 'product', 'name'],
        'hoat_chat': ['hoat_chat', 'molecule'],
        'ten_cong_ty': ['ten_cong_ty', 'manufacturer', 'ten_tap_doan'],
        'ham_luong': ['ham_luong', 'galenic', 'nong do'],
        'dang_bao_che': ['dang_bao_che', 'unit_measure', 'dang_dung'],
        'sku_full': ['ten_day_du', 'sku', 'product_name'] 
    }

    final_rename = {}
    current_cols = df.columns.tolist()
    for std, aliases in mapping_dict.items():
        for alias in aliases:
            if alias in current_cols:
                final_rename[alias] = std
                break
    
    if final_rename: df.rename(columns=final_rename, inplace=True)
    
    required = ['ma_vtma', 'ten_thuoc', 'ten_cong_ty', 'hoat_chat', 'ham_luong', 'dang_bao_che']
    for col in required:
        if col not in df.columns: df[col] = "" 
        df[col] = df[col].astype(str).replace('nan', '')

    df['norm_name'] = df['ten_thuoc'].apply(normalize_text)
    df['norm_brand'] = df['ten_cong_ty'].apply(normalize_text)
    df['norm_active'] = df['hoat_chat'].apply(normalize_text)
    df['norm_strength'] = df['ham_luong'].apply(normalize_text)
    df['norm_form'] = df['dang_bao_che'].apply(normalize_text)
    
    df['search_index'] = df.apply(lambda x: f"{x['norm_name']} {x['norm_active']} {x['norm_strength']}", axis=1)

    if 'sku_full' in df.columns and len(df['sku_full']) > 0:
        df['display_name'] = df['sku_full']
    else:
        df['display_name'] = df['ten_thuoc'] + " " + df['ham_luong']

    return df

# =============================================================================
# 5. CORE ENGINE (FUZZY LOGIC)
# =============================================================================
def calculate_detailed_score(input_str, row_data, ml_predicted_brand=None):
    norm_input = normalize_text(input_str)
    score_name = fuzz.token_set_ratio(norm_input, row_data['norm_name'])
    score_brand = fuzz.partial_ratio(row_data['norm_brand'], norm_input)
    
    score_active = 0
    if row_data['norm_active']: score_active = fuzz.token_set_ratio(row_data['norm_active'], norm_input)
    else: score_active = 50 

    input_nums = extract_numbers(input_str)
    row_nums = extract_numbers(row_data['ham_luong'])
    score_strength = 0
    if not row_nums or not input_nums: score_strength = 50
    else:
        intersection = input_nums.intersection(row_nums)
        if len(intersection) == len(input_nums) and len(intersection) == len(row_nums): score_strength = 100 
        elif len(intersection) > 0: score_strength = 40 
        else: score_strength = 0

    score_form = fuzz.partial_ratio(row_data['norm_form'], norm_input)
    
    base_score = (score_name*0.4) + (score_brand*0.2) + (score_active*0.2) + (score_strength*0.1) + (score_form*0.1)
    
    ml_bonus = 0
    if ml_predicted_brand and row_data['norm_brand']:
        if fuzz.token_set_ratio(normalize_text(ml_predicted_brand), row_data['norm_brand']) > 85:
            ml_bonus = 15

    return {
        'total': min(base_score + ml_bonus, 100),
        's_name': score_name, 's_brand': score_brand, 's_active': score_active,
        's_strength': score_strength, 's_form': score_form, 'ml_bonus': ml_bonus
    }

def get_candidates(input_text, db_df, limit=20):
    if 'search_index' not in db_df.columns:
        db_df['search_index'] = db_df.apply(lambda x: f"{x['norm_name']} {x['norm_active']} {x['norm_strength']}", axis=1)
    
    norm_input = normalize_text(input_text)
    candidates = process.extract(norm_input, db_df['search_index'], limit=limit, scorer=fuzz.token_set_ratio)
    indices = [x[2] for x in candidates]
    return db_df.iloc[indices].copy()

def search_product_v3(input_text, db_df, brain_model, min_score=50, top_n=3):
    predicted_brand = brain_model.predict_brand(input_text)
    subset = get_candidates(input_text, db_df, limit=50)
    
    results = []
    for idx, row in subset.iterrows():
        scores = calculate_detailed_score(input_text, row, ml_predicted_brand=predicted_brand)
        if scores['total'] >= min_score:
            results.append({
                'M√£ VTMA': row['ma_vtma'],
                'T√™n Thu·ªëc (SKU)': row['display_name'],
                'NSX': row['ten_cong_ty'],
                'H√†m L∆∞·ª£ng': row['ham_luong'], 
                'ƒêi·ªÉm T·ªïng': round(scores['total'], 1),
                'ƒêi·ªÉm T√™n (40%)': int(scores['s_name']),
                'ƒêi·ªÉm H√£ng (20%)': int(scores['s_brand']),
                'ƒêi·ªÉm Ho·∫°tCh·∫•t (20%)': int(scores['s_active']),
                'ƒêi·ªÉm H√†mL∆∞·ª£ng (10%)': int(scores['s_strength']),
                'ƒêi·ªÉm D·∫°ng (10%)': int(scores['s_form']),
                'AI Bonus': scores['ml_bonus']
            })
            
    results.sort(key=lambda x: x['ƒêi·ªÉm T·ªïng'], reverse=True)
    return results[:top_n]

# =============================================================================
# 6. GIAO DI·ªÜN STREAMLIT
# =============================================================================

if 'brain' not in st.session_state:
    st.session_state.brain = PharmaBrain()
    st.session_state.brain.load_model()

if 'db_vtma' not in st.session_state:
    with st.spinner("‚è≥ ƒêang t·∫£i d·ªØ li·ªáu chu·∫©n..."):
        df_loaded = load_master_data()
        if df_loaded is not None: st.session_state.db_vtma = df_loaded
        else: st.stop()

# --- SIDEBAR: C·∫§U H√åNH API ---
with st.sidebar:
    st.header("ü§ñ C·∫•u h√¨nh Gemini AI")
    api_key = st.text_input("Nh·∫≠p Google API Key", type="password")
    
    # N√∫t ch·ªçn Model
    model_option = st.radio(
        "Ch·ªçn Phi√™n b·∫£n AI:",
        ("Gemini 1.5 Flash (Nhanh)", "Gemini 1.5 Pro (Th√¥ng minh)"),
        index=0,
        help="Flash: T·ªëc ƒë·ªô cao. Pro: Suy lu·∫≠n s√¢u."
    )
    
    selected_model_name = 'gemini-1.5-flash' if "Flash" in model_option else 'gemini-1.5-pro'
    
    if api_key:
        st.session_state.gemini = GeminiAgent(api_key, selected_model_name)
        
        # Hi·ªÉn th·ªã model th·ª±c t·∫ø ƒëang d√πng (ƒë·ªÉ check l·ªói 404)
        if "gemini-pro" in st.session_state.gemini.current_model:
            st.warning(f"‚ö†Ô∏è ƒêang d√πng model d·ª± ph√≤ng: {st.session_state.gemini.current_model}")
        else:
            st.success(f"‚úÖ ƒê√£ k·∫øt n·ªëi: {st.session_state.gemini.current_model}")
    else:
        st.warning("‚ö†Ô∏è C·∫ßn API Key ƒë·ªÉ d√πng AI")
        st.session_state.gemini = GeminiAgent(None)

    st.divider()
    st.header("‚öôÔ∏è C·∫•u h√¨nh Map")
    min_score = st.slider("Min Score (%)", 0, 100, 60) 
    top_n = st.number_input("Top N", 1, 10, 3)
    
    st.subheader("‚öôÔ∏è C·∫•u h√¨nh AI R√† So√°t")
    threshold_ai = st.number_input("Ng∆∞·ª°ng k√≠ch ho·∫°t AI (xx)", 0, 100, 70)

st.title("üß¨ PharmaMaster: AI-Powered Mapping")

tab1, tab2 = st.tabs(["üöÄ Mapping & Gemini", "üß† Training Model"])

with tab1:
    st.subheader("Mapping D·ªØ Li·ªáu")
    uploaded = st.file_uploader("Upload file Excel", type=['xlsx', 'csv'])
    
    if uploaded:
        if uploaded.name.endswith('.csv'): df_in = pd.read_csv(uploaded)
        else: df_in = pd.read_excel(uploaded)
        
        st.write(f"ƒê√£ nh·∫≠n {len(df_in)} d√≤ng.")
        col_target = st.selectbox("Ch·ªçn c·ªôt T√™n thu·ªëc:", df_in.columns)
        
        if st.button("üöÄ B∆Ø·ªöC 1: CH·∫†Y MAPPING C∆† B·∫¢N"):
            all_results = []
            bar = st.progress(0)
            
            for i, row in df_in.iterrows():
                inp = str(row[col_target])
                matches = search_product_v3(inp, st.session_state.db_vtma, st.session_state.brain, min_score, top_n)
                
                if matches:
                    for rank, m in enumerate(matches, 1):
                        current_score = m['ƒêi·ªÉm T·ªïng']
                        status = "Kh·ªõp 100%" if current_score == 100 else ("Kh·ªõp cao" if rank == 1 else ("Trung b√¨nh" if current_score >= 70 else "Th·∫•p"))
                        
                        all_results.append({
                            'Input_Goc': inp, 'Rank': rank, 'Trang_Thai': status,
                            'Ma_VTMA': m['M√£ VTMA'], 'Ten_VTMA': m['T√™n Thu·ªëc (SKU)'],
                            'NSX_Chuan': m['NSX'], 'Ham_Luong_Chuan': m['H√†m L∆∞·ª£ng'],
                            'Diem_Tong': m['ƒêi·ªÉm T·ªïng'], 'AI_Suggestion': '' 
                        })
                else:
                    all_results.append({
                        'Input_Goc': inp, 'Rank': 1, 'Trang_Thai': 'Kh√¥ng t√¨m th·∫•y',
                        'Ma_VTMA': '', 'Ten_VTMA': '', 'NSX_Chuan': '', 'Ham_Luong_Chuan': '',
                        'Diem_Tong': 0, 'AI_Suggestion': ''
                    })
                bar.progress((i+1)/len(df_in))
            
            st.session_state.result_df = pd.DataFrame(all_results)
            st.success("‚úÖ ƒê√£ ch·∫°y xong Fuzzy Match c∆° b·∫£n!")

        if 'result_df' in st.session_state:
            st.divider()
            st.subheader("üõ†Ô∏è C√îNG C·ª§ AI N√ÇNG CAO")
            
            col_ai_1, col_ai_2 = st.columns(2)
            
            with col_ai_1:
                st.write("**Option 1: AI R√† So√°t**")
                st.caption("Check l·∫°i c√°c d√≤ng Rank 1 nghi ng·ªù.")
                if st.button("üïµÔ∏è K√≠ch ho·∫°t AI R√† So√°t"):
                    if not st.session_state.gemini.is_ready:
                        st.error("Thi·∫øu API Key!")
                    else:
                        df_res = st.session_state.result_df
                        target_rows = df_res[df_res['Rank'] == 1]
                        
                        my_bar = st.progress(0)
                        count = 0
                        
                        for idx, row in target_rows.iterrows():
                            if row['Diem_Tong'] < 90:
                                candidates = get_candidates(row['Input_Goc'], st.session_state.db_vtma, limit=15)
                                ai_response = st.session_state.gemini.smart_match(row['Input_Goc'], candidates)
                                st.session_state.result_df.at[idx, 'AI_Suggestion'] = f"ü§ñ {ai_response}"
                            count += 1
                            my_bar.progress(count / len(target_rows))
                        st.success("ƒê√£ r√† so√°t xong!")
                        st.session_state.result_df = st.session_state.result_df 

            with col_ai_2:
                st.write("**Option 2: Deep Search**")
                st.caption(f"T√¨m k·ªπ c√°c ca kh√≥ (ƒêi·ªÉm < {threshold_ai})")
                if st.button("üîç K√≠ch ho·∫°t Deep Search"):
                    if not st.session_state.gemini.is_ready:
                        st.error("Thi·∫øu API Key!")
                    else:
                        df_res = st.session_state.result_df
                        mask = (df_res['Diem_Tong'] < threshold_ai) | (df_res['Trang_Thai'] == 'Kh√¥ng t√¨m th·∫•y')
                        hard_cases = df_res[mask]
                        
                        if hard_cases.empty:
                            st.info("Kh√¥ng c√≥ ca kh√≥ n√†o.")
                        else:
                            my_bar = st.progress(0)
                            count = 0
                            for idx, row in hard_cases.iterrows():
                                candidates = get_candidates(row['Input_Goc'], st.session_state.db_vtma, limit=30)
                                ai_response = st.session_state.gemini.smart_match(row['Input_Goc'], candidates)
                                st.session_state.result_df.at[idx, 'AI_Suggestion'] = f"üîç {ai_response}"
                                count += 1
                                my_bar.progress(count / len(hard_cases))
                            st.success(f"ƒê√£ x·ª≠ l√Ω {len(hard_cases)} ca kh√≥!")
                            st.session_state.result_df = st.session_state.result_df 

            st.dataframe(st.session_state.result_df)
            df_final = st.session_state.result_df
            excel_name = "ket_qua_map_AI.xlsx"
            df_final.to_excel(excel_name, index=False)
            with open(excel_name, "rb") as f:
                st.download_button("üì• T·∫£i Excel (K√®m ƒë·ªÅ xu·∫•t AI)", f, excel_name)

with tab2:
    st.write("Ph·∫ßn Training AI (Gi·ªØ nguy√™n)...")

with tab2:
    st.write("Ph·∫ßn Training AI (Gi·ªØ nguy√™n)...")
