import streamlit as st
import pandas as pd
import google.generativeai as genai
from rapidfuzz import fuzz, process
import unidecode
import json
import os
import re
import time
from datetime import datetime

# --- 1. C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="PharmaMatch: Final Batch", layout="wide")
st.title("üöÄ PharmaMatch: C√¥ng c·ª• Map D∆∞·ª£c Ph·∫©m")

# --- 2. C√ÅC H√ÄM X·ª¨ L√ù ---
def normalize_text(text):
    if pd.isna(text): return ""
    return unidecode.unidecode(str(text).lower()).strip()

def extract_numbers(text):
    if pd.isna(text): return set()
    nums = re.findall(r"\d+\.?\d*", str(text))
    return set(nums)

# --- 3. LOAD DATA (C√≥ x·ª≠ l√Ω l·ªói nh∆∞ng KH√îNG D·ª™NG app ngay) ---
@st.cache_data
def load_vtma_data():
    try:
        # Ki·ªÉm tra c·∫£ ch·ªØ th∆∞·ªùng v√† hoa cho ch·∫Øc ƒÉn
        file_path = "data/vtma_standard.csv"
        if not os.path.exists(file_path):
             # Th·ª≠ t√¨m file vi·∫øt hoa n·∫øu user l·ª° ƒë·∫∑t t√™n kh√°c
            if os.path.exists("Data/vtma_standard.csv"): file_path = "Data/vtma_standard.csv"
            else: return None
            
        df = pd.read_csv(file_path)
        df['norm_name'] = df['ten_thuoc'].apply(normalize_text)
        df['norm_strength'] = df['ham_luong'].apply(normalize_text)
        df['norm_ingre'] = df['hoat_chat'].apply(normalize_text)
        df['norm_manu'] = df['ten_cong_ty'].apply(normalize_text)
        return df
    except Exception as e:
        return None

# --- 4. G·ªåI AI ---
def ai_process_batch(product_list, api_key):
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        items_str = "\n".join([f"- ID_{i}: {p}" for i, p in enumerate(product_list)])
        prompt = f"""
        Tr√≠ch xu·∫•t th√¥ng tin d∆∞·ª£c ph·∫©m:
        {items_str}
        Tr·∫£ v·ªÅ JSON List Objects:
        - "id": "ID_..."
        - "brand_name": T√™n bi·ªát d∆∞·ª£c
        - "strength": H√†m l∆∞·ª£ng s·ªë (VD: 500mg, 160/4.5).
        - "active_ingredient": Ho·∫°t ch·∫•t.
        - "manufacturer": T√™n h√£ng.
        """
        response = model.generate_content(prompt)
        text = response.text.replace('```json', '').replace('```', '').strip()
        data = json.loads(text)
        return {item['id']: item for item in data}
    except:
        return {}

# --- 5. LOGIC MAP ---
def hierarchical_match(input_data, vtma_df):
    if not input_data: return None, 0, "L·ªói AI"
    
    input_brand = normalize_text(input_data.get('brand_name', ''))
    input_strength = normalize_text(input_data.get('strength', ''))
    input_ingre = normalize_text(input_data.get('active_ingredient', ''))
    
    candidates = process.extract(input_brand, vtma_df['norm_name'], limit=30, scorer=fuzz.token_set_ratio)
    candidate_indices = [x[2] for x in candidates if x[1] >= 50]
    
    if not candidate_indices: return None, 0, "Kh√¥ng t√¨m th·∫•y t√™n"

    subset_df = vtma_df.iloc[candidate_indices].copy()
    results = []
    input_nums = extract_numbers(input_strength)
    
    for idx, row in subset_df.iterrows():
        name_score = fuzz.token_set_ratio(input_brand, row['norm_name']) * 0.4
        str_score = 0
        row_nums = extract_numbers(row['norm_strength'])
        
        if not input_nums: str_score = fuzz.ratio(input_strength, row['norm_strength']) * 0.4
        else:
            if input_nums.issubset(row_nums) or row_nums.issubset(input_nums): str_score = 40
            else: str_score = 0
        
        ing_score = fuzz.token_sort_ratio(input_ingre, row['norm_ingre']) * 0.2
        final_score = name_score + str_score + ing_score
        results.append({'row': row, 'score': final_score})
    
    results.sort(key=lambda x: x['score'], reverse=True)
    if results: return results[0]['row'], results[0]['score'], "OK"
    else: return None, 0, "Kh√¥ng KQ"

# --- 6. GIAO DI·ªÜN (ƒê√É CH·ªàNH S·ª¨A V·ªä TR√ç) ---

# C√†i ƒë·∫∑t Sidebar
with st.sidebar:
    st.header("C·∫•u h√¨nh")
    api_key = st.text_input("Gemini API Key", type="password")
    if not api_key and "GENAI_API_KEY" in st.secrets:
        api_key = st.secrets["GENAI_API_KEY"]
    batch_size = st.slider("Batch Size", 5, 20, 10)

# Load data ng·∫ßm
vtma_df = load_vtma_data()

# --- PH·∫¶N UPLOAD FILE (ƒê∆∞a l√™n ƒë·∫ßu trang ƒë·ªÉ lu√¥n nh√¨n th·∫•y) ---
st.subheader("1. T·∫£i danh m·ª•c D∆∞·ª£c V∆∞∆°ng")
uploaded = st.file_uploader("K√©o th·∫£ file v√†o ƒë√¢y (Excel/CSV)", type=['xlsx', 'csv'])

# --- N√öT CH·∫†Y V√Ä HI·ªÇN TH·ªä L·ªñI ---
if uploaded:
    # ƒê·ªçc file ngay ƒë·ªÉ user th·∫•y d·ªØ li·ªáu
    if uploaded.name.endswith('.csv'): df_in = pd.read_csv(uploaded)
    else: df_in = pd.read_excel(uploaded)
    
    st.write(f"ƒê√£ nh·∫≠n file: {len(df_in)} d√≤ng. C·ªôt s·∫Ω map: **{df_in.columns[0]}**")
    
    # N√∫t ch·∫°y
    if st.button("üöÄ CH·∫†Y MAPPING NGAY"):
        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán ch·∫°y l√∫c ·∫•n n√∫t
        if vtma_df is None or vtma_df.empty:
            st.error("‚ùå L·ªñI: Ch∆∞a t√¨m th·∫•y file d·ªØ li·ªáu chu·∫©n VTMA tr√™n h·ªá th·ªëng!")
            st.info("C√°ch s·ª≠a: H√£y ki·ªÉm tra tr√™n GitHub c·ªßa b·∫°n ƒë√£ c√≥ folder 'data' v√† file 'vtma_standard.csv' b√™n trong ch∆∞a.")
            st.stop()
            
        if not api_key:
            st.error("‚ùå Thi·∫øu API Key!")
            st.stop()

        # B·∫ÆT ƒê·∫¶U CH·∫†Y
        col_name = df_in.columns[0]
        final_results = []
        input_list = df_in[col_name].astype(str).tolist()
        total = len(input_list)
        bar = st.progress(0, text="ƒêang x·ª≠ l√Ω...")
        
        for i in range(0, total, batch_size):
            batch_items = input_list[i : i + batch_size]
            try: ai_data_dict = ai_process_batch(batch_items, api_key)
            except: ai_data_dict = {}
            
            for idx_in_batch, item_name in enumerate(batch_items):
                item_id = f"ID_{idx_in_batch}"
                ai_info = ai_data_dict.get(item_id, {})
                match_row, score, note = hierarchical_match(ai_info, vtma_df)
                
                res_row = {
                    'DV_Input': item_name,
                    'AI_Info': f"{ai_info.get('brand_name')} {ai_info.get('strength')}",
                    'VTMA_Code': '', 'VTMA_Name': '', 'VTMA_HamLuong': '',
                    'Score': score, 'Danh_Gia': 'Th·∫•p'
                }
                if match_row is not None:
                    res_row.update({
                        'VTMA_Code': match_row['ma_thuoc'],
                        'VTMA_Name': match_row['ten_thuoc'],
                        'VTMA_HamLuong': match_row['ham_luong'],
                        'Danh_Gia': 'Cao' if score > 75 else 'Ki·ªÉm tra'
                    })
                final_results.append(res_row)
            
            bar.progress(min((i + batch_size) / total, 1.0))
            time.sleep(1)

        st.success("Xong!")
        res_df = pd.DataFrame(final_results)
        st.dataframe(res_df)
        
        os.makedirs('output', exist_ok=True)
        fname = f"output/map_final_{datetime.now().strftime('%H%M')}.xlsx"
        res_df.to_excel(fname, index=False)
        with open(fname, "rb") as f:
            st.download_button("üì• T·∫£i k·∫øt qu·∫£", f, file_name="ket_qua.xlsx")

elif vtma_df is None:
    # N·∫øu ch∆∞a upload file input, th√¨ hi·ªán c·∫£nh b√°o nh·∫π v·ªÅ file VTMA n·∫øu thi·∫øu
    st.warning("‚ö†Ô∏è C·∫£nh b√°o: H·ªá th·ªëng ch∆∞a t√¨m th·∫•y file 'data/vtma_standard.csv'. B·∫°n v·∫´n c√≥ th·ªÉ th·∫•y n√∫t upload b√™n tr√™n, nh∆∞ng khi ch·∫°y s·∫Ω b√°o l·ªói.")
    
