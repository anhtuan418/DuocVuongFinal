import streamlit as st
import pandas as pd
from rapidfuzz import fuzz, process
import unidecode
import re
import os
import pickle
from collections import Counter

# =============================================================================
# 1. C·∫§U H√åNH TRANG
# =============================================================================
st.set_page_config(page_title="PharmaMaster: Final Fix", layout="wide", page_icon="üíä")

# =============================================================================
# 2. CLASS MACHINE LEARNING (GI·ªÆ NGUY√äN)
# =============================================================================
class PharmaBrain:
    def __init__(self):
        self.brand_memory = {} 
        self.learned_status = False

    def _tokenize(self, text):
        if pd.isna(text): return []
        text = unidecode.unidecode(str(text).lower())
        return re.findall(r"\w+", text)

    def learn(self, history_df, input_col, brand_col):
        brand_counter = {}
        count_learned = 0
        for _, row in history_df.iterrows():
            raw_text = row[input_col]
            true_brand = row[brand_col]
            if pd.isna(true_brand) or pd.isna(raw_text): continue
            tokens = self._tokenize(raw_text)
            for token in tokens:
                if len(token) < 2 or token.isdigit(): continue
                if token not in brand_counter: brand_counter[token] = Counter()
                brand_counter[token][true_brand] += 1

        self.brand_memory = {}
        for token, counts in brand_counter.items():
            most_common_brand, count = counts.most_common(1)[0]
            total = sum(counts.values())
            if total >= 2 and (count / total) > 0.7: 
                self.brand_memory[token] = most_common_brand
                count_learned += 1
        self.learned_status = True
        return count_learned

    def predict_brand(self, raw_text):
        if not self.brand_memory: return None
        tokens = self._tokenize(raw_text)
        detected_brands = []
        for token in tokens:
            if token in self.brand_memory:
                detected_brands.append(self.brand_memory[token])
        if detected_brands:
            return Counter(detected_brands).most_common(1)[0][0]
        return None

    def save_model(self):
        with open("pharma_brain.pkl", "wb") as f: pickle.dump(self.brand_memory, f)

    def load_model(self):
        if os.path.exists("pharma_brain.pkl"):
            with open("pharma_brain.pkl", "rb") as f: self.brand_memory = pickle.load(f)
            self.learned_status = True
            return True
        return False

# =============================================================================
# 3. X·ª¨ L√ù D·ªÆ LI·ªÜU & LOAD FILE
# =============================================================================

def normalize_text(text):
    if pd.isna(text): return ""
    return unidecode.unidecode(str(text).lower()).strip()

def extract_numbers(text):
    """
    Tr√≠ch xu·∫•t s·ªë th√¥ng minh. 
    Femoston 1/10 -> {1.0, 10.0}
    """
    if pd.isna(text): return set()
    # Thay c√°c k√Ω t·ª± ph√¢n c√°ch b·∫±ng kho·∫£ng tr·∫Øng ƒë·ªÉ t√°ch s·ªë d√≠nh nhau (1mg/5mg)
    clean_text = str(text).replace('/', ' ').replace('-', ' ').replace('+', ' ')
    # Regex l·∫•y s·ªë th·ª±c
    nums = re.findall(r"\d+\.?\d*", clean_text)
    # Chuy·ªÉn v·ªÅ float ƒë·ªÉ so s√°nh (1.0 == 1)
    return {float(n) for n in nums}

@st.cache_data
def load_master_data():
    file_path = "data/vtma_standard.csv"
    if not os.path.exists(file_path):
        st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file data t·∫°i: {file_path}")
        return None

    try:
        # ∆Øu ti√™n ƒë·ªçc utf-8-sig (ƒë·ªÉ x·ª≠ l√Ω BOM)
        df = pd.read_csv(file_path, sep=None, engine='python', encoding='utf-8-sig')
    except:
        try:
            df = pd.read_csv(file_path, sep=None, engine='python', encoding='latin1')
        except Exception as e:
            st.error(f"‚ùå L·ªói ƒë·ªçc file: {e}")
            return None

    # Chu·∫©n h√≥a t√™n c·ªôt: B·ªè BOM, ch·ªØ th∆∞·ªùng, b·ªè kho·∫£ng tr·∫Øng
    df.columns = df.columns.str.strip().str.lower().str.replace('\ufeff', '').str.replace('√Ø¬ª¬ø', '')
    
    # Mapping C·ªôt (Theo ·∫£nh image_f5fcd8.png c·ªßa b·∫°n)
    mapping_dict = {
        'ma_vtma': ['ma_thuoc', 'vtma code'],
        'ten_thuoc': ['ten_thuoc', 'product'],
        'hoat_chat': ['hoat_chat', 'molecule'],
        'ten_cong_ty': ['ten_cong_ty', 'manufacturer', 'ten_tap_doan'],
        'ham_luong': ['ham_luong', 'galenic'],
        'dang_bao_che': ['dang_bao_che', 'unit_measure', 'dang_dung'],
        'sku_full': ['ten_day_du', 'sku', 'product_name'] 
    }

    final_rename = {}
    current_cols = df.columns.tolist()
    for std, aliases in mapping_dict.items():
        found = False
        for alias in aliases:
            if alias in current_cols:
                final_rename[alias] = std
                found = True
                break
    
    if final_rename: df.rename(columns=final_rename, inplace=True)
    
    # T·∫°o c√°c c·ªôt chu·∫©n h√≥a
    required = ['ma_vtma', 'ten_thuoc', 'ten_cong_ty', 'hoat_chat', 'ham_luong', 'dang_bao_che']
    for col in required:
        if col not in df.columns: df[col] = "" # T·∫°o c·ªôt r·ªóng n·∫øu thi·∫øu
        df[col] = df[col].astype(str).replace('nan', '')

    df['norm_name'] = df['ten_thuoc'].apply(normalize_text)
    df['norm_brand'] = df['ten_cong_ty'].apply(normalize_text)
    df['norm_active'] = df['hoat_chat'].apply(normalize_text)
    df['norm_strength'] = df['ham_luong'].apply(normalize_text)
    df['norm_form'] = df['dang_bao_che'].apply(normalize_text)
    
    # Search Index ƒë·ªÉ l·ªçc s∆° b·ªô
    df['search_index'] = df.apply(lambda x: f"{x['norm_name']} {x['norm_active']} {x['norm_strength']}", axis=1)

    if 'sku_full' in df.columns and len(df['sku_full']) > 0:
        df['display_name'] = df['sku_full']
    else:
        df['display_name'] = df['ten_thuoc'] + " " + df['ham_luong']

    return df

# =============================================================================
# 4. THU·∫¨T TO√ÅN T√çNH ƒêI·ªÇM (CORE ENGINE - ƒê√É FIX FEMOSTON)
# =============================================================================

def calculate_detailed_score(input_str, row_data, ml_predicted_brand=None):
    norm_input = normalize_text(input_str)
    
    # 1. T√™n thu·ªëc (40%)
    score_name = fuzz.token_set_ratio(norm_input, row_data['norm_name'])
    
    # 2. H√£ng (20%)
    score_brand = fuzz.partial_ratio(row_data['norm_brand'], norm_input)
    
    # 3. Ho·∫°t ch·∫•t (20%)
    score_active = 0
    if row_data['norm_active']:
        score_active = fuzz.token_set_ratio(row_data['norm_active'], norm_input)
    else:
        score_active = 50 # Kh√¥ng c√≥ d·ªØ li·ªáu ho·∫°t ch·∫•t th√¨ cho ƒëi·ªÉm trung b√¨nh

    # 4. H√†m l∆∞·ª£ng (10%) - LOGIC M·ªöI CHO FEMOSTON 1/10
    input_nums = extract_numbers(input_str)
    row_nums = extract_numbers(row_data['ham_luong'])
    
    score_strength = 0
    if not row_nums:
        score_strength = 50
    elif not input_nums:
        score_strength = 50
    else:
        # Giao thoa s·ªë: Input {1, 10}, Row {1, 5} -> Giao {1} -> Sai
        # Input {1, 10}, Row {1, 10} -> Giao {1, 10} -> ƒê√∫ng
        intersection = input_nums.intersection(row_nums)
        
        if len(intersection) == len(input_nums) and len(intersection) == len(row_nums):
            score_strength = 100 # Kh·ªõp ho√†n to√†n b·ªô s·ªë
        elif len(intersection) > 0:
            # C√≥ kh·ªõp 1 ph·∫ßn (V√≠ d·ª• kh·ªõp s·ªë 1 nh∆∞ng l·ªách s·ªë 10)
            # PH·∫†T N·∫∂NG: N·∫øu s·ªë l∆∞·ª£ng s·ªë kh√°c nhau -> Tr·ª´ ƒëi·ªÉm
            score_strength = 40 
        else:
            score_strength = 0 # Kh√¥ng kh·ªõp s·ªë n√†o

    # 5. D·∫°ng b√†o ch·∫ø (10%)
    score_form = fuzz.partial_ratio(row_data['norm_form'], norm_input)
    
    # T·ªîNG H·ª¢P
    base_score = (score_name*0.4) + (score_brand*0.2) + (score_active*0.2) + (score_strength*0.1) + (score_form*0.1)
    
    # AI BONUS
    ml_bonus = 0
    if ml_predicted_brand and row_data['norm_brand']:
        if fuzz.token_set_ratio(normalize_text(ml_predicted_brand), row_data['norm_brand']) > 85:
            ml_bonus = 15

    final_score = min(base_score + ml_bonus, 100)

    # TR·∫¢ V·ªÄ DICTIONARY ƒê·ªÇ T√ÅCH C·ªòT
    return {
        'total': final_score,
        's_name': score_name,
        's_brand': score_brand,
        's_active': score_active,
        's_strength': score_strength,
        's_form': score_form,
        'ml_bonus': ml_bonus
    }

def search_product_v3(input_text, db_df, brain_model, min_score=50, top_n=3):
    predicted_brand = brain_model.predict_brand(input_text)
    norm_input = normalize_text(input_text)
    
    # B1: L·ªçc s∆° b·ªô (Search Index) - Quan tr·ªçng ƒë·ªÉ b·∫Øt Femoston
    candidates = process.extract(
        norm_input, 
        db_df['search_index'], 
        limit=100, 
        scorer=fuzz.token_set_ratio
    )
    
    # L·∫•y index ·ª©ng vi√™n (ng∆∞·ª°ng th·∫•p 30% ƒë·ªÉ kh√¥ng b·ªè s√≥t)
    candidate_indices = [x[2] for x in candidates if x[1] > 30]
    
    if not candidate_indices: return []

    subset = db_df.iloc[candidate_indices].copy()
    results = []
    
    # B2: Ch·∫•m ƒëi·ªÉm chi ti·∫øt
    for idx, row in subset.iterrows():
        scores = calculate_detailed_score(input_text, row, ml_predicted_brand=predicted_brand)
        
        if scores['total'] >= min_score:
            results.append({
                'M√£ VTMA': row['ma_vtma'],
                'T√™n Thu·ªëc (SKU)': row['display_name'],
                'NSX': row['ten_cong_ty'],
                'H√†m L∆∞·ª£ng': row['ham_luong'], # Hi·ªán th√™m c·ªôt n√†y ƒë·ªÉ check
                'ƒêi·ªÉm T·ªïng': round(scores['total'], 1),
                # C√°c c·ªôt ƒëi·ªÉm chi ti·∫øt
                'ƒêi·ªÉm T√™n (40%)': int(scores['s_name']),
                'ƒêi·ªÉm H√£ng (20%)': int(scores['s_brand']),
                'ƒêi·ªÉm Ho·∫°tCh·∫•t (20%)': int(scores['s_active']),
                'ƒêi·ªÉm H√†mL∆∞·ª£ng (10%)': int(scores['s_strength']),
                'ƒêi·ªÉm D·∫°ng (10%)': int(scores['s_form']),
                'AI Bonus': scores['ml_bonus']
            })
            
    results.sort(key=lambda x: x['ƒêi·ªÉm T·ªïng'], reverse=True)
    return results[:top_n]

# =============================================================================
# 5. GIAO DI·ªÜN CH√çNH
# =============================================================================

if 'brain' not in st.session_state:
    st.session_state.brain = PharmaBrain()
    st.session_state.brain.load_model()

if 'db_vtma' not in st.session_state:
    with st.spinner("‚è≥ ƒêang t·∫£i d·ªØ li·ªáu chu·∫©n (VTMA)..."):
        df_loaded = load_master_data()
        if df_loaded is not None: st.session_state.db_vtma = df_loaded
        else: st.stop()

with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")
    # ƒêi·ªÅu ch·ªânh m·∫∑c ƒë·ªãnh v·ªÅ 60 ƒë·ªÉ l·ªçc b·ªõt r√°c
    min_score = st.slider("Min Score (%)", 0, 100, 60) 
    top_n = st.number_input("Top N (S·ªë k·∫øt qu·∫£)", 1, 10, 3)
    st.info(f"Database: {len(st.session_state.db_vtma)} SKU")

st.title("üíä PharmaMaster: Final Edition (Font Fix + Multi-Columns)")

tab1, tab2 = st.tabs(["üöÄ Mapping & B√°o C√°o", "üß† Training AI"])

with tab1:
    st.subheader("Mapping File Excel")
    uploaded = st.file_uploader("Upload file Excel c·∫ßn map", type=['xlsx', 'csv'])
    
    if uploaded:
        if uploaded.name.endswith('.csv'): df_in = pd.read_csv(uploaded)
        else: df_in = pd.read_excel(uploaded)
        
        st.write(f"ƒê√£ nh·∫≠n {len(df_in)} d√≤ng.")
        col_target = st.selectbox("Ch·ªçn c·ªôt T√™n thu·ªëc:", df_in.columns)
        
        if st.button("üöÄ CH·∫†Y MAPPING"):
            all_results = []
            bar = st.progress(0)
            
            for i, row in df_in.iterrows():
                inp = str(row[col_target])
                matches = search_product_v3(inp, st.session_state.db_vtma, st.session_state.brain, min_score, top_n)
                
                if matches:
                    for rank, m in enumerate(matches, 1):
                        all_results.append({
                            'Input_Goc': inp,
                            'Rank': rank,
                            'Ma_VTMA': m['M√£ VTMA'],
                            'Ten_VTMA': m['T√™n Thu·ªëc (SKU)'],
                            'NSX_Chuan': m['NSX'],
                            'Ham_Luong_Chuan': m['H√†m L∆∞·ª£ng'],
                            'Diem_Tong': m['ƒêi·ªÉm T·ªïng'],
                            # T√°ch th√†nh 5 c·ªôt nh∆∞ y√™u c·∫ßu
                            'Diem_Ten': m['ƒêi·ªÉm T√™n (40%)'],
                            'Diem_Hang': m['ƒêi·ªÉm H√£ng (20%)'],
                            'Diem_HoatChat': m['ƒêi·ªÉm Ho·∫°tCh·∫•t (20%)'],
                            'Diem_HamLuong': m['ƒêi·ªÉm H√†mL∆∞·ª£ng (10%)'],
                            'Diem_Dang': m['ƒêi·ªÉm D·∫°ng (10%)'],
                            'AI_Bonus': m['AI Bonus']
                        })
                else:
                    # D√≤ng tr·ªëng n·∫øu kh√¥ng t√¨m th·∫•y
                    empty_row = {
                        'Input_Goc': inp, 'Rank': '-', 'Ma_VTMA': 'Kh√¥ng t√¨m th·∫•y',
                        'Ten_VTMA': '', 'NSX_Chuan': '', 'Ham_Luong_Chuan': '',
                        'Diem_Tong': 0, 'Diem_Ten':0, 'Diem_Hang':0, 'Diem_HoatChat':0,
                        'Diem_HamLuong':0, 'Diem_Dang':0, 'AI_Bonus':0
                    }
                    all_results.append(empty_row)
                
                bar.progress((i+1)/len(df_in))
                
            df_out = pd.DataFrame(all_results)
            st.success("‚úÖ Ho√†n t·∫•t!")
            
            # Hi·ªÉn th·ªã
            st.dataframe(df_out, use_container_width=True)
            
            # Xu·∫•t Excel
            excel_name = "ket_qua_map_final.xlsx"
            df_out.to_excel(excel_name, index=False)
            with open(excel_name, "rb") as f:
                st.download_button("üì• T·∫£i Excel (Chu·∫©n font)", f, excel_name)
                
            # Xu·∫•t CSV (FIX L·ªñI FONT ·ªû ƒê√ÇY)
            csv = df_out.to_csv(index=False, encoding='utf-8-sig') # Quan tr·ªçng: utf-8-sig
            st.download_button("üì• T·∫£i CSV (Chu·∫©n font)", csv, "ket_qua_map_final.csv", "text/csv")

with tab2:
    st.write("Ph·∫ßn Training AI (Gi·ªØ nguy√™n)...")
