import streamlit as st
import pandas as pd
from rapidfuzz import fuzz, process
import unidecode
import re
import os
import pickle
from collections import Counter
import google.generativeai as genai
import time
import random
import io

# =============================================================================
# 1. C·∫§U H√åNH TRANG & CSS
# =============================================================================
st.set_page_config(page_title="PharmaMaster Ultimate", layout="wide", page_icon="üß¨")

# =============================================================================
# 2. CLASS GEMINI AI (V·ªöI C∆† CH·∫æ RETRY M·∫†NH M·∫º T·ª™ FILE 02)
# =============================================================================
class GeminiAgent:
    def __init__(self, api_key, model_name):
        self.is_ready = False
        self.current_model = "None"
        
        if api_key and model_name:
            try:
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel(model_name)
                self.current_model = model_name
                self.is_ready = True
            except Exception as e:
                self.is_ready = False
                self.error = str(e)
        else:
            self.is_ready = False

    def smart_match(self, input_drug, candidates_df):
        """
        G·ª≠i y√™u c·∫ßu v·ªõi c∆° ch·∫ø Retry (Th·ª≠ l·∫°i) khi g·∫∑p l·ªói 429
        """
        if not self.is_ready: return "‚ö†Ô∏è L·ªói: Ch∆∞a ch·ªçn Model ho·∫∑c API Key sai"

        candidates_str = ""
        for idx, row in candidates_df.iterrows():
            candidates_str += f"- ID: {row['ma_vtma']} | T√™n: {row['ten_thuoc']} | HL: {row['ham_luong']} | NSX: {row['nsx_full']}\n"

        prompt = f"""
        B·∫°n l√† D∆∞·ª£c sƒ©. T√¨m m√£ thu·ªëc chu·∫©n (ID) cho s·∫£n ph·∫©m ƒë·∫ßu v√†o.
        INPUT: "{input_drug}"
        DATABASE:
        {candidates_str}
        Y√äU C·∫¶U: Ch·ªçn 1 ID kh·ªõp nh·∫•t.
        TR·∫¢ L·ªúI 1 D√íNG DUY NH·∫§T: ID_CHON | ƒê·ªò_TIN_C·∫¨Y (Th·∫•p/Trung b√¨nh/Cao) | L√ù DO NG·∫ÆN G·ªåN
        V√≠ d·ª•: VTMA_001 | Cao | Kh·ªõp t√™n v√† h√£ng
        N·∫øu kh√¥ng kh·ªõp >70%, tr·∫£ v·ªÅ: "NONE | - | Kh√¥ng t√¨m th·∫•y"
        """
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.model.generate_content(prompt)
                return response.text.strip()
            except Exception as e:
                error_str = str(e)
                if "429" in error_str or "quota" in error_str.lower():
                    wait_time = (attempt + 1) * 3 + random.uniform(0, 2)
                    time.sleep(wait_time)
                    continue
                else:
                    return f"AI Error: {error_str}"
        
        return "‚ö†Ô∏è AI Busy (H·∫øt h·∫°n m·ª©c, vui l√≤ng ch·ªù 1 ph√∫t)"

# =============================================================================
# 3. CLASS MACHINE LEARNING (PHARMA BRAIN - GI·ªÆ NGUY√äN)
# =============================================================================
class PharmaBrain:
    def __init__(self):
        self.brand_memory = {} 
        self.learned_status = False

    def _tokenize(self, text):
        if pd.isna(text): return []
        text = unidecode.unidecode(str(text).lower())
        return re.findall(r"\w+", text)

    def learn(self, history_df, input_col, brand_col):
        brand_counter = {}
        count_learned = 0
        for _, row in history_df.iterrows():
            raw_text = row[input_col]
            true_brand = row[brand_col]
            if pd.isna(true_brand) or pd.isna(raw_text): continue
            tokens = self._tokenize(raw_text)
            for token in tokens:
                if len(token) < 2 or token.isdigit(): continue
                if token not in brand_counter: brand_counter[token] = Counter()
                brand_counter[token][true_brand] += 1

        self.brand_memory = {}
        for token, counts in brand_counter.items():
            most_common_brand, count = counts.most_common(1)[0]
            total = sum(counts.values())
            if total >= 2 and (count / total) > 0.7: 
                self.brand_memory[token] = most_common_brand
                count_learned += 1
        self.learned_status = True
        return count_learned

    def predict_brand(self, raw_text):
        if not self.brand_memory: return None
        tokens = self._tokenize(raw_text)
        detected_brands = []
        for token in tokens:
            if token in self.brand_memory:
                detected_brands.append(self.brand_memory[token])
        if detected_brands:
            return Counter(detected_brands).most_common(1)[0][0]
        return None

    def save_model(self):
        with open("pharma_brain.pkl", "wb") as f: pickle.dump(self.brand_memory, f)

    def load_model(self):
        if os.path.exists("pharma_brain.pkl"):
            with open("pharma_brain.pkl", "rb") as f: self.brand_memory = pickle.load(f)
            self.learned_status = True
            return True
        return False

# =============================================================================
# 4. X·ª¨ L√ù D·ªÆ LI·ªÜU T·ªêI ∆ØU (MERGE LOGIC)
# =============================================================================
def normalize_text(text):
    if pd.isna(text): return ""
    return unidecode.unidecode(str(text).lower()).strip()

def extract_numbers(text):
    if pd.isna(text): return set()
    clean_text = str(text).replace('/', ' ').replace('-', ' ').replace('+', ' ')
    nums = re.findall(r"\d+\.?\d*", clean_text)
    return {float(n) for n in nums}

@st.cache_data
def load_master_data():
    file_path = "data/vtma_standard.csv"
    if not os.path.exists(file_path):
        st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file data t·∫°i: {file_path}")
        return None

    try:
        df = pd.read_csv(file_path, sep=None, engine='python', encoding='utf-8-sig')
    except:
        try: df = pd.read_csv(file_path, sep=None, engine='python', encoding='latin1')
        except Exception as e:
            st.error(f"‚ùå L·ªói ƒë·ªçc file: {e}")
            return None

    df.columns = df.columns.str.strip().str.lower().str.replace('\ufeff', '').str.replace('√Ø¬ª¬ø', '')
    
    # 1. Mapping c·ªôt th√¥ng minh (T·ª´ File 01)
    mapping_dict = {
        'ma_vtma': ['ma_thuoc', 'vtma code', 'code'],
        'ten_thuoc': ['ten_thuoc', 'product', 'name'],
        'hoat_chat': ['hoat_chat', 'molecule'],
        'ten_cong_ty': ['ten_cong_ty', 'manufacturer', 'ten_tap_doan'],
        'corporation': ['corporation', 'tap_doan'],
        'ham_luong': ['ham_luong', 'galenic', 'nong do'],
        'dang_bao_che': ['dang_bao_che', 'unit_measure'],
        'sku_full': ['ten_day_du', 'sku', 'product_name'] 
    }

    final_rename = {}
    current_cols = df.columns.tolist()
    for std, aliases in mapping_dict.items():
        for alias in aliases:
            if alias in current_cols:
                final_rename[alias] = std
                break
    
    if final_rename: df.rename(columns=final_rename, inplace=True)
    
    required = ['ma_vtma', 'ten_thuoc', 'ten_cong_ty', 'hoat_chat', 'ham_luong', 'dang_bao_che']
    for col in required:
        if col not in df.columns: df[col] = "" 
        df[col] = df[col].astype(str).replace('nan', '')

    # 2. X·ª≠ l√Ω g·ªôp c·ªôt NSX (T·ª´ File 01 - Quan tr·ªçng cho l·ªçc)
    if 'corporation' in df.columns:
        df['nsx_full'] = df['ten_cong_ty'] + " (" + df['corporation'].fillna('') + ")"
    else:
        df['nsx_full'] = df['ten_cong_ty']
    df['nsx_full'] = df['nsx_full'].str.replace(r'\(\s*\)', '', regex=True).str.strip()

    # 3. T√≠nh to√°n tr∆∞·ªõc c√°c c·ªôt chu·∫©n h√≥a (T·ª´ File 02 - T·ªëi ∆∞u t·ªëc ƒë·ªô)
    df['norm_name'] = df['ten_thuoc'].apply(normalize_text)
    df['norm_brand'] = df['ten_cong_ty'].apply(normalize_text) # V·∫´n gi·ªØ norm_brand g·ªëc ƒë·ªÉ so s√°nh t√™n
    df['norm_active'] = df['hoat_chat'].apply(normalize_text)
    df['norm_strength'] = df['ham_luong'].apply(normalize_text)
    df['norm_form'] = df['dang_bao_che'].apply(normalize_text)
    
    df['search_index'] = df.apply(lambda x: f"{x['norm_name']} {x['norm_active']} {x['norm_strength']}", axis=1)

    if 'sku_full' in df.columns and len(df['sku_full']) > 0:
        df['display_name'] = df['sku_full']
    else:
        df['display_name'] = df['ten_thuoc'] + " " + df['ham_luong']

    return df

# =============================================================================
# 5. CORE ENGINE (K·∫æT H·ª¢P LOGIC)
# =============================================================================
def calculate_detailed_score(input_str, row_data, ml_predicted_brand=None):
    norm_input = normalize_text(input_str)
    
    # D√πng c·ªôt ƒë√£ chu·∫©n h√≥a s·∫µn (t·ªëi ∆∞u t·ª´ File 02)
    score_name = fuzz.token_set_ratio(norm_input, row_data['norm_name'])
    score_brand = fuzz.partial_ratio(row_data['norm_brand'], norm_input)
    
    score_active = 0
    if row_data['norm_active']: score_active = fuzz.token_set_ratio(row_data['norm_active'], norm_input)
    else: score_active = 50 

    # Logic s·ªë h·ªçc (T·ª´ c·∫£ 2 file)
    input_nums = extract_numbers(input_str)
    row_nums = extract_numbers(row_data['ham_luong'])
    score_strength = 0
    if not row_nums or not input_nums: score_strength = 50
    else:
        intersection = input_nums.intersection(row_nums)
        if len(intersection) == len(input_nums) and len(intersection) == len(row_nums): score_strength = 100 
        elif len(intersection) > 0: score_strength = 40 
        else: score_strength = 0

    score_form = fuzz.partial_ratio(row_data['norm_form'], norm_input)
    
    base_score = (score_name*0.4) + (score_brand*0.2) + (score_active*0.2) + (score_strength*0.1) + (score_form*0.1)
    
    ml_bonus = 0
    if ml_predicted_brand and row_data['norm_brand']:
        if fuzz.token_set_ratio(normalize_text(ml_predicted_brand), row_data['norm_brand']) > 85:
            ml_bonus = 15

    return {
        'total': min(base_score + ml_bonus, 100),
        's_name': score_name, 's_brand': score_brand, 's_active': score_active,
        's_strength': score_strength, 's_form': score_form, 'ml_bonus': ml_bonus
    }

def get_candidates(input_text, db_df, limit=20, filtered_nsx=None):
    # Logic l·ªçc NSX (T·ª´ File 01)
    working_df = db_df
    if filtered_nsx:
        # L·ªçc dataframe tr∆∞·ªõc khi fuzzy search -> TƒÉng t·ªëc & Ch√≠nh x√°c c·ª±c cao
        working_df = db_df[db_df['nsx_full'].isin(filtered_nsx)]
    
    if working_df.empty: return pd.DataFrame()

    norm_input = normalize_text(input_text)
    # Search tr√™n t·∫≠p ƒë√£ l·ªçc
    candidates = process.extract(norm_input, working_df['search_index'], limit=limit, scorer=fuzz.token_set_ratio)
    indices = [x[2] for x in candidates]
    return working_df.iloc[indices].copy()

def search_product_v3(input_text, db_df, brain_model, min_score=50, top_n=3, filtered_nsx=None):
    predicted_brand = brain_model.predict_brand(input_text)
    subset = get_candidates(input_text, db_df, limit=50, filtered_nsx=filtered_nsx)
    
    if subset.empty: return []

    results = []
    for idx, row in subset.iterrows():
        scores = calculate_detailed_score(input_text, row, ml_predicted_brand=predicted_brand)
        if scores['total'] >= min_score:
            results.append({
                'M√£ VTMA': row['ma_vtma'],
                'T√™n Thu·ªëc (SKU)': row['display_name'],
                'NSX': row['nsx_full'],
                'H√†m L∆∞·ª£ng': row['ham_luong'], 
                'ƒêi·ªÉm T·ªïng': round(scores['total'], 1),
                'ƒêi·ªÉm T√™n (40%)': int(scores['s_name']),
                'ƒêi·ªÉm H√£ng (20%)': int(scores['s_brand']),
                'ƒêi·ªÉm Ho·∫°tCh·∫•t (20%)': int(scores['s_active']),
                'ƒêi·ªÉm H√†mL∆∞·ª£ng (10%)': int(scores['s_strength']),
                'AI Bonus': scores['ml_bonus']
            })
            
    results.sort(key=lambda x: x['ƒêi·ªÉm T·ªïng'], reverse=True)
    return results[:top_n]

# =============================================================================
# 6. GIAO DI·ªÜN STREAMLIT (MERGE WORKFLOW)
# =============================================================================

if 'brain' not in st.session_state:
    st.session_state.brain = PharmaBrain()
    st.session_state.brain.load_model()

if 'db_vtma' not in st.session_state:
    with st.spinner("‚è≥ ƒêang t·∫£i d·ªØ li·ªáu chu·∫©n..."):
        df_loaded = load_master_data()
        if df_loaded is not None: st.session_state.db_vtma = df_loaded
        else: st.stop()

# Kh·ªüi t·∫°o session
if 'confirmed_nsx' not in st.session_state: st.session_state.confirmed_nsx = []
if 'brand_step_skipped' not in st.session_state: st.session_state.brand_step_skipped = False
if 'brand_suggestions' not in st.session_state: st.session_state.brand_suggestions = []

# --- SIDEBAR T·ª™ FILE 02 (CLEANER) ---
with st.sidebar:
    st.header("ü§ñ C·∫•u h√¨nh Gemini AI")
    api_key = st.text_input("Nh·∫≠p Google API Key", type="password")
    valid_models = []
    if api_key:
        try:
            genai.configure(api_key=api_key)
            all_models = genai.list_models()
            for m in all_models:
                if 'generateContent' in m.supported_generation_methods:
                     valid_models.append(m.name.replace("models/", ""))
        except: st.error("API Key l·ªói!")

    if valid_models:
        default_ix = valid_models.index('gemini-1.5-flash') if 'gemini-1.5-flash' in valid_models else 0
        selected_model = st.selectbox("Ch·ªçn Model AI:", valid_models, index=default_ix)
        st.session_state.gemini = GeminiAgent(api_key, selected_model)
        st.success("‚úÖ AI S·∫µn s√†ng")
    else:
        st.info("Nh·∫≠p API Key ƒë·ªÉ d√πng t√≠nh nƒÉng AI s·ª≠a l·ªói.")
        st.session_state.gemini = GeminiAgent(None, None)

    st.divider()
    st.header("‚öôÔ∏è C·∫•u h√¨nh Map")
    min_score = st.slider("Min Score (%)", 0, 100, 60) 
    top_n = st.number_input("Top N", 1, 10, 3)
    threshold_ai = st.number_input("Ng∆∞·ª°ng k√≠ch ho·∫°t Deep Search", 0, 100, 70)

st.title("üß¨ PharmaMaster Ultimate: Intelligent Mapping")

# --- TAB WORKFLOW: K·∫æT H·ª¢P 3 B∆Ø·ªöC ---
tab1, tab_brand, tab3, tab4 = st.tabs(["1Ô∏è‚É£ Upload & Test", "2Ô∏è‚É£ Ch·ªçn B·ªô L·ªçc (NSX)", "3Ô∏è‚É£ Ch·∫°y Full & Fix L·ªói", "4Ô∏è‚É£ Training Model"])

# --- TAB 1: UPLOAD & TEST SAMPLE (T·ª™ FILE 01) ---
with tab1:
    st.subheader("B∆∞·ªõc 1: T·∫£i d·ªØ li·ªáu & Ph√¢n t√≠ch m·∫´u")
    uploaded = st.file_uploader("Upload file Excel/CSV c·∫ßn map", type=['xlsx', 'csv'])
    
    if uploaded:
        if uploaded.name.endswith('.csv'): df_in = pd.read_csv(uploaded)
        else: df_in = pd.read_excel(uploaded)
        
        st.session_state.df_input = df_in # L∆∞u v√†o session ƒë·ªÉ d√πng cho Tab 3
        st.write(f"ƒê√£ nh·∫≠n {len(df_in)} d√≤ng d·ªØ li·ªáu.")
        col_target = st.selectbox("Ch·ªçn c·ªôt T√™n thu·ªëc:", df_in.columns, key="col_target")
        st.session_state.col_target = col_target

        if st.button("üß™ CH·∫†Y TH·ª¨ 3 M·∫™U & G·ª¢I √ù NSX"):
            sample_3 = df_in.head(3)
            temp_results = []
            for i, row in sample_3.iterrows():
                inp = str(row[col_target])
                # Ch·∫°y kh√¥ng l·ªçc ƒë·ªÉ t√¨m NSX ti·ªÅm nƒÉng
                matches = search_product_v3(inp, st.session_state.db_vtma, st.session_state.brain, 30, 1)
                if matches:
                    temp_results.append({'Input': inp, 'NSX_G·ª£i_√ù': matches[0]['NSX'], 'M√£': matches[0]['M√£ VTMA']})
            
            st.session_state.brand_suggestions = temp_results
            st.success("‚úÖ ƒê√£ xong! H√£y chuy·ªÉn sang Tab 'Ch·ªçn B·ªô L·ªçc' ƒë·ªÉ x√°c nh·∫≠n c√°c NSX n√†y.")
            st.table(temp_results)

# --- TAB 2: BRAND FILTER (T·ª™ FILE 01 - T√çNH NƒÇNG "S√ÅT TH·ª¶") ---
with tab_brand:
    st.subheader("B∆∞·ªõc 2: X√°c nh·∫≠n Nh√† S·∫£n Xu·∫•t (B·ªô l·ªçc)")
    st.info("üí° Vi·ªác l·ªçc ƒë√∫ng NSX s·∫Ω gi√∫p lo·∫°i b·ªè 90% k·∫øt qu·∫£ sai v√† tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω.")

    # 1. Hi·ªÉn th·ªã g·ª£i √Ω t·ª´ b∆∞·ªõc 1
    if st.session_state.brand_suggestions:
        suggestions = list(set([item['NSX_G·ª£i_√ù'] for item in st.session_state.brand_suggestions]))
        st.write("G·ª£i √Ω t·ª´ d·ªØ li·ªáu m·∫´u:")
        for nsx in suggestions:
            c1, c2 = st.columns([4, 1])
            c1.info(f"üè≠ {nsx}")
            if c2.button("Th√™m", key=f"add_{nsx}"):
                if nsx not in st.session_state.confirmed_nsx:
                    st.session_state.confirmed_nsx.append(nsx)
                    st.rerun()

    st.divider()
    
    # 2. Ch·ªçn th·ªß c√¥ng
    all_vtma_nsx = sorted(st.session_state.db_vtma['nsx_full'].unique().tolist())
    selected_manual = st.selectbox("T√¨m & Th√™m th·ªß c√¥ng:", ["--- Ch·ªçn nh√† m√°y ---"] + all_vtma_nsx)
    if st.button("‚ûï Th√™m v√†o danh s√°ch"):
        if selected_manual != "--- Ch·ªçn nh√† m√°y ---" and selected_manual not in st.session_state.confirmed_nsx:
            st.session_state.confirmed_nsx.append(selected_manual)
            st.rerun()

    # 3. Danh s√°ch ƒë√£ ch·ªçn
    st.write("### üìã Danh s√°ch √°p d·ª•ng:")
    if st.session_state.confirmed_nsx:
        for nsx in st.session_state.confirmed_nsx:
            st.success(f"‚úÖ {nsx}")
        if st.button("üóëÔ∏è X√≥a t·∫•t c·∫£ b·ªô l·ªçc"):
            st.session_state.confirmed_nsx = []
            st.session_state.brand_step_skipped = False
            st.rerun()
    else:
        st.warning("Ch∆∞a c√≥ b·ªô l·ªçc n√†o.")

    if st.checkbox("‚è© B·ªè qua b∆∞·ªõc n√†y (T√¨m tr√™n to√†n b·ªô Database)", value=st.session_state.brand_step_skipped):
        st.session_state.brand_step_skipped = True
    else:
        st.session_state.brand_step_skipped = False

# --- TAB 3: FULL RUN & AI FIX (K·∫æT H·ª¢P FILE 01 & 02) ---
with tab3:
    st.subheader("B∆∞·ªõc 3: Ch·∫°y Mapping & AI H·∫≠u Ki·ªÉm")
    
    if 'df_input' not in st.session_state:
        st.error("Vui l√≤ng upload file ·ªü Tab 1 tr∆∞·ªõc.")
    else:
        # N√∫t ch·∫°y ch√≠nh
        if st.button("üöÄ CH·∫†Y FULL MAPPING"):
            filter_list = st.session_state.confirmed_nsx if not st.session_state.brand_step_skipped else None
            
            all_results = []
            bar = st.progress(0)
            df_run = st.session_state.df_input
            col_t = st.session_state.col_target

            for i, row in df_run.iterrows():
                inp = str(row[col_t])
                # G·ªçi h√†m search v·ªõi filter_list
                matches = search_product_v3(inp, st.session_state.db_vtma, st.session_state.brain, min_score, top_n, filtered_nsx=filter_list)
                
                if matches:
                    for rank, m in enumerate(matches, 1):
                        all_results.append({
                            'Input_Goc': inp, 'Rank': rank, 'Trang_Thai': 'Kh·ªõp',
                            'Ma_VTMA': m['M√£ VTMA'], 'Ten_VTMA': m['T√™n Thu·ªëc (SKU)'],
                            'NSX_Chuan': m['NSX'],'Ham_Luong_Chuan': m['H√†m L∆∞·ª£ng'],
                            'Diem_Tong': m['ƒêi·ªÉm T·ªïng'], 'AI_Suggestion': '' 
                        })
                else:
                    # Tr∆∞·ªùng h·ª£p kh√¥ng t√¨m th·∫•y (Not Found)
                    all_results.append({
                        'Input_Goc': inp, 'Rank': 1, 'Trang_Thai': 'Kh√¥ng t√¨m th·∫•y',
                        'Ma_VTMA': '', 'Ten_VTMA': '', 'NSX_Chuan': '', 'Ham_Luong_Chuan': '',
                        'Diem_Tong': 0, 'AI_Suggestion': ''
                    })
                
                # C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh
                bar.progress((i+1)/len(df_run))
            
            # L∆∞u k·∫øt qu·∫£ v√†o Session State
            st.session_state.result_df = pd.DataFrame(all_results)
            st.success("‚úÖ ƒê√£ ch·∫°y xong Fuzzy Match c∆° b·∫£n!")

    # --- KHU V·ª∞C 2: AI DEEP SEARCH & DOWNLOAD (T·ª™ FILE 02) ---
    if 'result_df' in st.session_state:
        st.divider()
        st.subheader("üõ†Ô∏è C√¥ng c·ª•: AI R√† So√°t & Deep Search")
        
        col_ai_1, col_ai_2 = st.columns([2, 1])
        
        with col_ai_1:
            st.info(f"AI s·∫Ω t·ª± ƒë·ªông ki·ªÉm tra c√°c d√≤ng c√≥ ƒêi·ªÉm < {threshold_ai} ho·∫∑c 'Kh√¥ng t√¨m th·∫•y'.")
            
            if st.button("üïµÔ∏è K√≠ch ho·∫°t AI R√† So√°t (Deep Search)"):
                if not st.session_state.gemini.is_ready:
                    st.error("‚ùå Thi·∫øu API Key! Vui l√≤ng nh·∫≠p Key ·ªü c·ªôt b√™n tr√°i.")
                else:
                    df_res = st.session_state.result_df
                    # L·ªçc ra c√°c ca kh√≥ c·∫ßn AI x·ª≠ l√Ω
                    mask = (df_res['Diem_Tong'] < threshold_ai) | (df_res['Trang_Thai'] == 'Kh√¥ng t√¨m th·∫•y')
                    # Ch·ªâ l·∫•y Rank 1 ƒë·ªÉ check (tr√°nh check tr√πng l·∫∑p c√°c rank sau)
                    hard_cases = df_res[mask & (df_res['Rank'] == 1)]
                    
                    if hard_cases.empty:
                        st.success("üéâ D·ªØ li·ªáu qu√° t·ªët! Kh√¥ng c√≥ d√≤ng n√†o d∆∞·ªõi ng∆∞·ª°ng ƒëi·ªÉm c·∫ßn AI s·ª≠a.")
                    else:
                        st.write(f"ƒêang x·ª≠ l√Ω {len(hard_cases)} tr∆∞·ªùng h·ª£p nghi ng·ªù...")
                        my_bar = st.progress(0)
                        count = 0
                        
                        # S·ª≠ d·ª•ng filter hi·ªán t·∫°i n·∫øu c√≥
                        current_filter = st.session_state.confirmed_nsx if not st.session_state.brand_step_skipped else None

                        for idx, row in hard_cases.iterrows():
                            # L·∫•y candidates r·ªông h∆°n (limit=20) ƒë·ªÉ AI c√≥ nhi·ªÅu l·ª±a ch·ªçn
                            candidates = get_candidates(row['Input_Goc'], st.session_state.db_vtma, limit=20, filtered_nsx=current_filter)
                            
                            # G·ªçi Gemini Agent (ƒë√£ c√≥ Retry logic)
                            ai_response = st.session_state.gemini.smart_match(row['Input_Goc'], candidates)
                            
                            # Ghi k·∫øt qu·∫£ v√†o c·ªôt AI Suggestion
                            st.session_state.result_df.at[idx, 'AI_Suggestion'] = f"ü§ñ {ai_response}"
                            
                            # Delay nh·∫π ƒë·ªÉ tr√°nh l·ªói 429 n·∫øu ch·∫°y qu√° nhanh
                            time.sleep(1.5)
                            
                            count += 1
                            my_bar.progress(count / len(hard_cases))
                        
                        st.success(f"‚úÖ ƒê√£ r√† so√°t xong {len(hard_cases)} d√≤ng!")
                        st.rerun() # Load l·∫°i trang ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£ m·ªõi

        with col_ai_2:
            st.write("### üì• Xu·∫•t K·∫øt Qu·∫£")
            # Hi·ªÉn th·ªã dataframe k·∫øt qu·∫£
            st.dataframe(st.session_state.result_df, height=300)
            
            # Logic xu·∫•t Excel
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                st.session_state.result_df.to_excel(writer, index=False, sheet_name='KetQua')
            
            st.download_button(
                label="T·∫£i file Excel (.xlsx)",
                data=buffer,
                file_name="Pharma_Map_Result_AI.xlsx",
                mime="application/vnd.ms-excel"
            )

# --- TAB 4: TRAINING MODEL (T·ª™ FILE 01 - GI√öP M√ÅY KH√îN H∆†N) ---
with tab4:
    st.subheader("4Ô∏è‚É£ Hu·∫•n luy·ªán AI (Supervised Learning)")
    st.info("N·∫øu m√°y nh·∫≠n di·ªán sai h√£ng (v√≠ d·ª•: 'DHG' kh√¥ng ra 'D∆∞·ª£c H·∫≠u Giang'), h√£y upload file l·ªãch s·ª≠ ƒë√£ map ƒë√∫ng ƒë·ªÉ d·∫°y l·∫°i m√°y.")
    
    uploaded_hist = st.file_uploader("Ch·ªçn file l·ªãch s·ª≠ mapping (.xlsx)", key="hist")
    
    if uploaded_hist:
        df_hist = pd.read_excel(uploaded_hist)
        st.write("D·ªØ li·ªáu m·∫´u:")
        st.dataframe(df_hist.head(3))
        
        c1, c2 = st.columns(2)
        col_in = c1.selectbox("C·ªôt T√™n G·ªëc (Input) - V√≠ d·ª•: Ten_Thuoc", df_hist.columns)
        col_out = c2.selectbox("C·ªôt H√£ng Chu·∫©n (Target) - V√≠ d·ª•: NSX_Chuan", df_hist.columns)
        
        if st.button("üéì B·∫ÆT ƒê·∫¶U D·∫†Y M√ÅY"):
            with st.spinner("ƒêang ph√¢n t√≠ch quy lu·∫≠t t·ª´ ng·ªØ..."):
                # G·ªçi h√†m learn t·ª´ Class PharmaBrain
                n_learned = st.session_state.brain.learn(df_hist, col_in, col_out)
                st.session_state.brain.save_model()
            
            st.success(f"üéâ ƒê√£ h·ªçc xong! M√°y ƒë√£ ghi nh·ªõ th√™m {n_learned} t·ª´ kh√≥a nh·∫≠n di·ªán h√£ng m·ªõi.")
            
            with st.expander("Xem b·ªô nh·ªõ (Brand Memory)"):
                st.json(st.session_state.brain.brand_memory)
