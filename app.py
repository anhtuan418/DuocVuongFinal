import streamlit as st
import pandas as pd
from rapidfuzz import fuzz, process
import unidecode
import re
import os
import pickle
from collections import Counter
import google.generativeai as genai
import time
import random
import io

# =============================================================================
# 1. Cáº¤U HÃŒNH TRANG & CSS
# =============================================================================
st.set_page_config(page_title="PharmaMaster Ultimate", layout="wide", page_icon="ğŸ§¬")

# =============================================================================
# 2. CLASS GEMINI AI (Vá»šI CÆ  CHáº¾ RETRY Máº NH Máº¼ Tá»ª FILE 02)
# =============================================================================
class GeminiAgent:
    def __init__(self, api_key, model_name):
        self.is_ready = False
        self.current_model = "None"
        
        if api_key and model_name:
            try:
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel(model_name)
                self.current_model = model_name
                self.is_ready = True
            except Exception as e:
                self.is_ready = False
                self.error = str(e)
        else:
            self.is_ready = False

    def smart_match(self, input_drug, candidates_df):
        """
        Gá»­i yÃªu cáº§u vá»›i cÆ¡ cháº¿ Retry (Thá»­ láº¡i) khi gáº·p lá»—i 429
        """
        if not self.is_ready: return "âš ï¸ Lá»—i: ChÆ°a chá»n Model hoáº·c API Key sai"

        candidates_str = ""
        for idx, row in candidates_df.iterrows():
            candidates_str += f"- ID: {row['ma_vtma']} | TÃªn: {row['ten_thuoc']} | HL: {row['ham_luong']} | NSX: {row['nsx_full']}\n"

        prompt = f"""
        Báº¡n lÃ  DÆ°á»£c sÄ©. TÃ¬m mÃ£ thuá»‘c chuáº©n (ID) cho sáº£n pháº©m Ä‘áº§u vÃ o.
        INPUT: "{input_drug}"
        DATABASE:
        {candidates_str}
        YÃŠU Cáº¦U: Chá»n 1 ID khá»›p nháº¥t.
        TRáº¢ Lá»œI 1 DÃ’NG DUY NHáº¤T: ID_CHON | Äá»˜_TIN_Cáº¬Y (Tháº¥p/Trung bÃ¬nh/Cao) | LÃ DO NGáº®N Gá»ŒN
        VÃ­ dá»¥: VTMA_001 | Cao | Khá»›p tÃªn vÃ  hÃ£ng
        Náº¿u khÃ´ng khá»›p >70%, tráº£ vá»: "NONE | - | KhÃ´ng tÃ¬m tháº¥y"
        """
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.model.generate_content(prompt)
                return response.text.strip()
            except Exception as e:
                error_str = str(e)
                if "429" in error_str or "quota" in error_str.lower():
                    wait_time = (attempt + 1) * 3 + random.uniform(0, 2)
                    time.sleep(wait_time)
                    continue
                else:
                    return f"AI Error: {error_str}"
        
        return "âš ï¸ AI Busy (Háº¿t háº¡n má»©c, vui lÃ²ng chá» 1 phÃºt)"

# =============================================================================
# 3. CLASS MACHINE LEARNING (PHARMA BRAIN - GIá»® NGUYÃŠN)
# =============================================================================
class PharmaBrain:
    def __init__(self):
        self.brand_memory = {} 
        self.learned_status = False

    def _tokenize(self, text):
        if pd.isna(text): return []
        text = unidecode.unidecode(str(text).lower())
        return re.findall(r"\w+", text)

    def learn(self, history_df, input_col, brand_col):
        brand_counter = {}
        count_learned = 0
        for _, row in history_df.iterrows():
            raw_text = row[input_col]
            true_brand = row[brand_col]
            if pd.isna(true_brand) or pd.isna(raw_text): continue
            tokens = self._tokenize(raw_text)
            for token in tokens:
                if len(token) < 2 or token.isdigit(): continue
                if token not in brand_counter: brand_counter[token] = Counter()
                brand_counter[token][true_brand] += 1

        self.brand_memory = {}
        for token, counts in brand_counter.items():
            most_common_brand, count = counts.most_common(1)[0]
            total = sum(counts.values())
            if total >= 2 and (count / total) > 0.7: 
                self.brand_memory[token] = most_common_brand
                count_learned += 1
        self.learned_status = True
        return count_learned

    def predict_brand(self, raw_text):
        if not self.brand_memory: return None
        tokens = self._tokenize(raw_text)
        detected_brands = []
        for token in tokens:
            if token in self.brand_memory:
                detected_brands.append(self.brand_memory[token])
        if detected_brands:
            return Counter(detected_brands).most_common(1)[0][0]
        return None

    def save_model(self):
        with open("pharma_brain.pkl", "wb") as f: pickle.dump(self.brand_memory, f)

    def load_model(self):
        if os.path.exists("pharma_brain.pkl"):
            with open("pharma_brain.pkl", "rb") as f: self.brand_memory = pickle.load(f)
            self.learned_status = True
            return True
        return False

# =============================================================================
# 4. Xá»¬ LÃ Dá»® LIá»†U Tá»I Æ¯U (MERGE LOGIC)
# =============================================================================
def normalize_text(text):
    if pd.isna(text): return ""
    return unidecode.unidecode(str(text).lower()).strip()

def extract_numbers(text):
    if pd.isna(text): return set()
    clean_text = str(text).replace('/', ' ').replace('-', ' ').replace('+', ' ')
    nums = re.findall(r"\d+\.?\d*", clean_text)
    return {float(n) for n in nums}

@st.cache_data
def load_master_data():
    file_path = "data/vtma_standard.csv"
    if not os.path.exists(file_path):
        st.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y file data táº¡i: {file_path}")
        return None

    try:
        df = pd.read_csv(file_path, sep=None, engine='python', encoding='utf-8-sig')
    except:
        try: df = pd.read_csv(file_path, sep=None, engine='python', encoding='latin1')
        except Exception as e:
            st.error(f"âŒ Lá»—i Ä‘á»c file: {e}")
            return None

    df.columns = df.columns.str.strip().str.lower().str.replace('\ufeff', '').str.replace('Ã¯Â»Â¿', '')
    
    # 1. Mapping cá»™t thÃ´ng minh (Tá»« File 01)
    mapping_dict = {
        'ma_vtma': ['ma_thuoc', 'vtma code', 'code'],
        'ten_thuoc': ['ten_thuoc', 'product', 'name'],
        'hoat_chat': ['hoat_chat', 'molecule'],
        'ten_cong_ty': ['ten_cong_ty', 'manufacturer', 'ten_tap_doan'],
        'corporation': ['corporation', 'tap_doan'],
        'ham_luong': ['ham_luong', 'galenic', 'nong do'],
        'dang_bao_che': ['dang_bao_che', 'unit_measure'],
        'sku_full': ['ten_day_du', 'sku', 'product_name'] 
    }

    final_rename = {}
    current_cols = df.columns.tolist()
    for std, aliases in mapping_dict.items():
        for alias in aliases:
            if alias in current_cols:
                final_rename[alias] = std
                break
    
    if final_rename: df.rename(columns=final_rename, inplace=True)
    
    required = ['ma_vtma', 'ten_thuoc', 'ten_cong_ty', 'hoat_chat', 'ham_luong', 'dang_bao_che']
    for col in required:
        if col not in df.columns: df[col] = "" 
        df[col] = df[col].astype(str).replace('nan', '')

    # 2. Xá»­ lÃ½ gá»™p cá»™t NSX (Tá»« File 01 - Quan trá»ng cho lá»c)
    if 'corporation' in df.columns:
        df['nsx_full'] = df['ten_cong_ty'] + " (" + df['corporation'].fillna('') + ")"
    else:
        df['nsx_full'] = df['ten_cong_ty']
    df['nsx_full'] = df['nsx_full'].str.replace(r'\(\s*\)', '', regex=True).str.strip()

    # 3. TÃ­nh toÃ¡n trÆ°á»›c cÃ¡c cá»™t chuáº©n hÃ³a (Tá»« File 02 - Tá»‘i Æ°u tá»‘c Ä‘á»™)
    df['norm_name'] = df['ten_thuoc'].apply(normalize_text)
    df['norm_brand'] = df['ten_cong_ty'].apply(normalize_text) # Váº«n giá»¯ norm_brand gá»‘c Ä‘á»ƒ so sÃ¡nh tÃªn
    df['norm_active'] = df['hoat_chat'].apply(normalize_text)
    df['norm_strength'] = df['ham_luong'].apply(normalize_text)
    df['norm_form'] = df['dang_bao_che'].apply(normalize_text)
    
    df['search_index'] = df.apply(lambda x: f"{x['norm_name']} {x['norm_active']} {x['norm_strength']}", axis=1)

    if 'sku_full' in df.columns and len(df['sku_full']) > 0:
        df['display_name'] = df['sku_full']
    else:
        df['display_name'] = df['ten_thuoc'] + " " + df['ham_luong']

    return df

# =============================================================================
# 5. CORE ENGINE (Káº¾T Há»¢P LOGIC)
# =============================================================================
def calculate_detailed_score(input_str, row_data, ml_predicted_brand=None):
    norm_input = normalize_text(input_str)
    
    # DÃ¹ng cá»™t Ä‘Ã£ chuáº©n hÃ³a sáºµn (tá»‘i Æ°u tá»« File 02)
    score_name = fuzz.token_set_ratio(norm_input, row_data['norm_name'])
    score_brand = fuzz.partial_ratio(row_data['norm_brand'], norm_input)
    
    score_active = 0
    if row_data['norm_active']: score_active = fuzz.token_set_ratio(row_data['norm_active'], norm_input)
    else: score_active = 50 

    # Logic sá»‘ há»c (Tá»« cáº£ 2 file)
    input_nums = extract_numbers(input_str)
    row_nums = extract_numbers(row_data['ham_luong'])
    score_strength = 0
    if not row_nums or not input_nums: score_strength = 50
    else:
        intersection = input_nums.intersection(row_nums)
        if len(intersection) == len(input_nums) and len(intersection) == len(row_nums): score_strength = 100 
        elif len(intersection) > 0: score_strength = 40 
        else: score_strength = 0

    score_form = fuzz.partial_ratio(row_data['norm_form'], norm_input)
    
    base_score = (score_name*0.4) + (score_brand*0.2) + (score_active*0.2) + (score_strength*0.1) + (score_form*0.1)
    
    ml_bonus = 0
    if ml_predicted_brand and row_data['norm_brand']:
        if fuzz.token_set_ratio(normalize_text(ml_predicted_brand), row_data['norm_brand']) > 85:
            ml_bonus = 15

    return {
        'total': min(base_score + ml_bonus, 100),
        's_name': score_name, 's_brand': score_brand, 's_active': score_active,
        's_strength': score_strength, 's_form': score_form, 'ml_bonus': ml_bonus
    }

def get_candidates(input_text, db_df, limit=20, filtered_nsx=None):
    # Logic lá»c NSX (Tá»« File 01)
    working_df = db_df
    if filtered_nsx:
        # Lá»c dataframe trÆ°á»›c khi fuzzy search -> TÄƒng tá»‘c & ChÃ­nh xÃ¡c cá»±c cao
        working_df = db_df[db_df['nsx_full'].isin(filtered_nsx)]
    
    if working_df.empty: return pd.DataFrame()

    norm_input = normalize_text(input_text)
    # Search trÃªn táº­p Ä‘Ã£ lá»c
    candidates = process.extract(norm_input, working_df['search_index'], limit=limit, scorer=fuzz.token_set_ratio)
    indices = [x[2] for x in candidates]
    return working_df.iloc[indices].copy()

def search_product_v3(input_text, db_df, brain_model, min_score=50, top_n=3, filtered_nsx=None):
    predicted_brand = brain_model.predict_brand(input_text)
    subset = get_candidates(input_text, db_df, limit=50, filtered_nsx=filtered_nsx)
    
    if subset.empty: return []

    results = []
    for idx, row in subset.iterrows():
        scores = calculate_detailed_score(input_text, row, ml_predicted_brand=predicted_brand)
        if scores['total'] >= min_score:
            results.append({
                'MÃ£ VTMA': row['ma_vtma'],
                'TÃªn Thuá»‘c (SKU)': row['display_name'],
                'NSX': row['nsx_full'],
                'HÃ m LÆ°á»£ng': row['ham_luong'], 
                'Äiá»ƒm Tá»•ng': round(scores['total'], 1),
                'Äiá»ƒm TÃªn (40%)': int(scores['s_name']),
                'Äiá»ƒm HÃ£ng (20%)': int(scores['s_brand']),
                'Äiá»ƒm Hoáº¡tCháº¥t (20%)': int(scores['s_active']),
                'Äiá»ƒm HÃ mLÆ°á»£ng (10%)': int(scores['s_strength']),
                'AI Bonus': scores['ml_bonus']
            })
            
    results.sort(key=lambda x: x['Äiá»ƒm Tá»•ng'], reverse=True)
    return results[:top_n]

# =============================================================================
# 6. GIAO DIá»†N STREAMLIT (MERGE WORKFLOW)
# =============================================================================

if 'brain' not in st.session_state:
    st.session_state.brain = PharmaBrain()
    st.session_state.brain.load_model()

if 'db_vtma' not in st.session_state:
    with st.spinner("â³ Äang táº£i dá»¯ liá»‡u chuáº©n..."):
        df_loaded = load_master_data()
        if df_loaded is not None: st.session_state.db_vtma = df_loaded
        else: st.stop()

# Khá»Ÿi táº¡o session
if 'confirmed_nsx' not in st.session_state: st.session_state.confirmed_nsx = []
if 'brand_step_skipped' not in st.session_state: st.session_state.brand_step_skipped = False
if 'brand_suggestions' not in st.session_state: st.session_state.brand_suggestions = []

# --- SIDEBAR Tá»ª FILE 02 (CLEANER) ---
with st.sidebar:
    st.header("ğŸ¤– Cáº¥u hÃ¬nh Gemini AI")
    api_key = st.text_input("Nháº­p Google API Key", type="password")
    valid_models = []
    if api_key:
        try:
            genai.configure(api_key=api_key)
            all_models = genai.list_models()
            for m in all_models:
                if 'generateContent' in m.supported_generation_methods:
                     valid_models.append(m.name.replace("models/", ""))
        except: st.error("API Key lá»—i!")

    if valid_models:
        default_ix = valid_models.index('gemini-1.5-flash') if 'gemini-1.5-flash' in valid_models else 0
        selected_model = st.selectbox("Chá»n Model AI:", valid_models, index=default_ix)
        st.session_state.gemini = GeminiAgent(api_key, selected_model)
        st.success("âœ… AI Sáºµn sÃ ng")
    else:
        st.info("Nháº­p API Key Ä‘á»ƒ dÃ¹ng tÃ­nh nÄƒng AI sá»­a lá»—i.")
        st.session_state.gemini = GeminiAgent(None, None)

    st.divider()
    st.header("âš™ï¸ Cáº¥u hÃ¬nh Map")
    min_score = st.slider("Min Score (%)", 0, 100, 60) 
    top_n = st.number_input("Top N", 1, 10, 3)
    threshold_ai = st.number_input("NgÆ°á»¡ng kÃ­ch hoáº¡t Deep Search", 0, 100, 70)

st.title("ğŸ§¬ PharmaMaster Ultimate: Intelligent Mapping")

# --- TAB WORKFLOW: Káº¾T Há»¢P 3 BÆ¯á»šC ---
tab1, tab_brand, tab3, tab4 = st.tabs(["1ï¸âƒ£ Upload & Test", "2ï¸âƒ£ Chá»n Bá»™ Lá»c (NSX)", "3ï¸âƒ£ Cháº¡y Full & Fix Lá»—i", "4ï¸âƒ£ Training Model"])

# --- TAB 1: UPLOAD & TEST SAMPLE (Tá»ª FILE 01) ---

with tab1:
    st.subheader("BÆ°á»›c 1: Nháº­p dá»¯ liá»‡u (Upload hoáº·c DÃ¡n)")
    
    # CÃCH 1: UPLOAD FILE
    uploaded = st.file_uploader("ğŸ“ CÃ¡ch 1: Upload file Excel/CSV", type=['xlsx', 'csv'])
    
    # CÃCH 2: DÃN Tá»ª EXCEL
    st.write("--- HOáº¶C ---")
    paste_text = st.text_area("ğŸ“‹ CÃ¡ch 2: Copy tá»« Excel vÃ  DÃ¡n vÃ o Ä‘Ã¢y (Ctrl+V)", height=150, 
                              placeholder="Má»Ÿ file Excel -> BÃ´i Ä‘en vÃ¹ng dá»¯ liá»‡u -> Ctrl+C -> Báº¥m vÃ o Ä‘Ã¢y vÃ  Ctrl+V")

    df_in = None

    # Xá»¬ LÃ Dá»® LIá»†U Äáº¦U VÃ€O
    if uploaded:
        try:
            if uploaded.name.endswith('.csv'): df_in = pd.read_csv(uploaded)
            else: df_in = pd.read_excel(uploaded)
            st.success(f"âœ… ÄÃ£ táº£i file: {uploaded.name}")
        except Exception as e:
            st.error(f"Lá»—i Ä‘á»c file: {e}")

    elif paste_text:
        try:
            # Excel copy thÆ°á»ng lÃ  dáº¡ng TSV (Tab Separated Values)
            df_in = pd.read_csv(io.StringIO(paste_text), sep='\t')
            st.success("âœ… ÄÃ£ Ä‘á»c dá»¯ liá»‡u tá»« Clipboard!")
        except:
            st.error("âŒ Dá»¯ liá»‡u dÃ¡n khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng báº£ng Excel.")

    # Náº¾U CÃ“ Dá»® LIá»†U (Tá»ª FILE HOáº¶C PASTE)
    if df_in is not None and not df_in.empty:
        # LÆ°u vÃ o Session State Ä‘á»ƒ Tab 3 dÃ¹ng
        st.session_state.df_input = df_in 
        st.write(f"ğŸ“Š Tá»•ng sá»‘ dÃ²ng: {len(df_in)}")
        st.dataframe(df_in.head(3)) # Show 3 dÃ²ng Ä‘áº§u check
        
        # Chá»n cá»™t (Code Ä‘Ã£ fix lá»—i Session State)
        # Chá»‰ dÃ¹ng key, khÃ´ng gÃ¡n thá»§ cÃ´ng st.session_state.col_target = ...
        col_target_box = st.selectbox("Chá»n cá»™t TÃªn thuá»‘c:", df_in.columns, key="col_target_key")
        
        # LÆ°u giÃ¡ trá»‹ cá»™t Ä‘Ã£ chá»n vÃ o biáº¿n riÃªng Ä‘á»ƒ dÃ¹ng (náº¿u cáº§n)
        st.session_state.col_target = col_target_box

        # NÃºt báº¥m cháº¡y thá»­
        if st.button("ğŸ§ª CHáº Y THá»¬ 3 MáºªU & Gá»¢I Ã NSX"):
            sample_3 = df_in.head(3)
            temp_results = []
            
            # Progress bar giáº£ láº­p cho Ä‘áº¹p
            with st.spinner("Äang phÃ¢n tÃ­ch máº«u..."):
                for i, row in sample_3.iterrows():
                    inp = str(row[col_target_box]) 
                    matches = search_product_v3(inp, st.session_state.db_vtma, st.session_state.brain, 30, 1)
                    if matches:
                        temp_results.append({'Input': inp, 'NSX_Gá»£i_Ã': matches[0]['NSX'], 'MÃ£': matches[0]['MÃ£ VTMA']})
            
            st.session_state.brand_suggestions = temp_results
            st.success("âœ… ÄÃ£ xong! HÃ£y chuyá»ƒn sang Tab 2 Ä‘á»ƒ xÃ¡c nháº­n bá»™ lá»c.")
            st.table(temp_results)
    else:
        st.info("ğŸ‘ˆ Vui lÃ²ng Upload file hoáº·c DÃ¡n dá»¯ liá»‡u Ä‘á»ƒ báº¯t Ä‘áº§u.")

# --- TAB 2: BRAND FILTER (Tá»ª FILE 01 - TÃNH NÄ‚NG "SÃT THá»¦") ---
with tab_brand:
    st.subheader("BÆ°á»›c 2: XÃ¡c nháº­n NhÃ  Sáº£n Xuáº¥t (Bá»™ lá»c)")
    st.info("ğŸ’¡ Viá»‡c lá»c Ä‘Ãºng NSX sáº½ giÃºp loáº¡i bá» 90% káº¿t quáº£ sai vÃ  tÄƒng tá»‘c Ä‘á»™ xá»­ lÃ½.")

    # 1. Hiá»ƒn thá»‹ gá»£i Ã½ tá»« bÆ°á»›c 1
    if st.session_state.brand_suggestions:
        suggestions = list(set([item['NSX_Gá»£i_Ã'] for item in st.session_state.brand_suggestions]))
        st.write("Gá»£i Ã½ tá»« dá»¯ liá»‡u máº«u:")
        for nsx in suggestions:
            c1, c2 = st.columns([4, 1])
            c1.info(f"ğŸ­ {nsx}")
            if c2.button("ThÃªm", key=f"add_{nsx}"):
                if nsx not in st.session_state.confirmed_nsx:
                    st.session_state.confirmed_nsx.append(nsx)
                    st.rerun()

    st.divider()
    
    # 2. Chá»n thá»§ cÃ´ng
    all_vtma_nsx = sorted(st.session_state.db_vtma['nsx_full'].unique().tolist())
    selected_manual = st.selectbox("TÃ¬m & ThÃªm thá»§ cÃ´ng:", ["--- Chá»n nhÃ  mÃ¡y ---"] + all_vtma_nsx)
    if st.button("â• ThÃªm vÃ o danh sÃ¡ch"):
        if selected_manual != "--- Chá»n nhÃ  mÃ¡y ---" and selected_manual not in st.session_state.confirmed_nsx:
            st.session_state.confirmed_nsx.append(selected_manual)
            st.rerun()

    # 3. Danh sÃ¡ch Ä‘Ã£ chá»n
    st.write("### ğŸ“‹ Danh sÃ¡ch Ã¡p dá»¥ng:")
    if st.session_state.confirmed_nsx:
        for nsx in st.session_state.confirmed_nsx:
            st.success(f"âœ… {nsx}")
        if st.button("ğŸ—‘ï¸ XÃ³a táº¥t cáº£ bá»™ lá»c"):
            st.session_state.confirmed_nsx = []
            st.session_state.brand_step_skipped = False
            st.rerun()
    else:
        st.warning("ChÆ°a cÃ³ bá»™ lá»c nÃ o.")

    if st.checkbox("â© Bá» qua bÆ°á»›c nÃ y (TÃ¬m trÃªn toÃ n bá»™ Database)", value=st.session_state.brand_step_skipped):
        st.session_state.brand_step_skipped = True
    else:
        st.session_state.brand_step_skipped = False

# --- TAB 3: FULL RUN & AI FIX (Káº¾T Há»¢P FILE 01 & 02) ---
with tab3:
    st.subheader("BÆ°á»›c 3: Cháº¡y Mapping & AI Háº­u Kiá»ƒm")
    
    if 'df_input' not in st.session_state:
        st.error("Vui lÃ²ng upload file á»Ÿ Tab 1 trÆ°á»›c.")
    else:
        # NÃºt cháº¡y chÃ­nh
        if st.button("ğŸš€ CHáº Y FULL MAPPING"):
            filter_list = st.session_state.confirmed_nsx if not st.session_state.brand_step_skipped else None
            
            all_results = []
            bar = st.progress(0)
            df_run = st.session_state.df_input
            col_t = st.session_state.col_target

            for i, row in df_run.iterrows():
                inp = str(row[col_t])
                # Gá»i hÃ m search vá»›i filter_list
                matches = search_product_v3(inp, st.session_state.db_vtma, st.session_state.brain, min_score, top_n, filtered_nsx=filter_list)
                
                if matches:
                    for rank, m in enumerate(matches, 1):
                        all_results.append({
                            'Input_Goc': inp, 'Rank': rank, 'Trang_Thai': 'Khá»›p',
                            'Ma_VTMA': m['MÃ£ VTMA'], 'Ten_VTMA': m['TÃªn Thuá»‘c (SKU)'],
                            'NSX_Chuan': m['NSX'],'Ham_Luong_Chuan': m['HÃ m LÆ°á»£ng'],
                            'Diem_Tong': m['Äiá»ƒm Tá»•ng'], 'AI_Suggestion': '' 
                        })
                else:
                    # TrÆ°á»ng há»£p khÃ´ng tÃ¬m tháº¥y (Not Found)
                    all_results.append({
                        'Input_Goc': inp, 'Rank': 1, 'Trang_Thai': 'KhÃ´ng tÃ¬m tháº¥y',
                        'Ma_VTMA': '', 'Ten_VTMA': '', 'NSX_Chuan': '', 'Ham_Luong_Chuan': '',
                        'Diem_Tong': 0, 'AI_Suggestion': ''
                    })
                
                # Cáº­p nháº­t thanh tiáº¿n trÃ¬nh
                bar.progress((i+1)/len(df_run))
            
            # LÆ°u káº¿t quáº£ vÃ o Session State
            st.session_state.result_df = pd.DataFrame(all_results)
            st.success("âœ… ÄÃ£ cháº¡y xong Fuzzy Match cÆ¡ báº£n!")

    # --- KHU Vá»°C 2: AI DEEP SEARCH & DOWNLOAD (Tá»ª FILE 02) ---
    if 'result_df' in st.session_state:
        st.divider()
        st.subheader("ğŸ› ï¸ CÃ´ng cá»¥: AI RÃ  SoÃ¡t & Deep Search")
        
        col_ai_1, col_ai_2 = st.columns([2, 1])
        
        with col_ai_1:
            st.info(f"AI sáº½ tá»± Ä‘á»™ng kiá»ƒm tra cÃ¡c dÃ²ng cÃ³ Äiá»ƒm < {threshold_ai} hoáº·c 'KhÃ´ng tÃ¬m tháº¥y'.")
            
            if st.button("ğŸ•µï¸ KÃ­ch hoáº¡t AI RÃ  SoÃ¡t (Deep Search)"):
                if not st.session_state.gemini.is_ready:
                    st.error("âŒ Thiáº¿u API Key! Vui lÃ²ng nháº­p Key á»Ÿ cá»™t bÃªn trÃ¡i.")
                else:
                    df_res = st.session_state.result_df
                    # Lá»c ra cÃ¡c ca khÃ³ cáº§n AI xá»­ lÃ½
                    mask = (df_res['Diem_Tong'] < threshold_ai) | (df_res['Trang_Thai'] == 'KhÃ´ng tÃ¬m tháº¥y')
                    # Chá»‰ láº¥y Rank 1 Ä‘á»ƒ check (trÃ¡nh check trÃ¹ng láº·p cÃ¡c rank sau)
                    hard_cases = df_res[mask & (df_res['Rank'] == 1)]
                    
                    if hard_cases.empty:
                        st.success("ğŸ‰ Dá»¯ liá»‡u quÃ¡ tá»‘t! KhÃ´ng cÃ³ dÃ²ng nÃ o dÆ°á»›i ngÆ°á»¡ng Ä‘iá»ƒm cáº§n AI sá»­a.")
                    else:
                        st.write(f"Äang xá»­ lÃ½ {len(hard_cases)} trÆ°á»ng há»£p nghi ngá»...")
                        my_bar = st.progress(0)
                        count = 0
                        
                        # Sá»­ dá»¥ng filter hiá»‡n táº¡i náº¿u cÃ³
                        current_filter = st.session_state.confirmed_nsx if not st.session_state.brand_step_skipped else None

                        for idx, row in hard_cases.iterrows():
                            # Láº¥y candidates rá»™ng hÆ¡n (limit=20) Ä‘á»ƒ AI cÃ³ nhiá»u lá»±a chá»n
                            candidates = get_candidates(row['Input_Goc'], st.session_state.db_vtma, limit=20, filtered_nsx=current_filter)
                            
                            # Gá»i Gemini Agent (Ä‘Ã£ cÃ³ Retry logic)
                            ai_response = st.session_state.gemini.smart_match(row['Input_Goc'], candidates)
                            
                            # Ghi káº¿t quáº£ vÃ o cá»™t AI Suggestion
                            st.session_state.result_df.at[idx, 'AI_Suggestion'] = f"ğŸ¤– {ai_response}"
                            
                            # Delay nháº¹ Ä‘á»ƒ trÃ¡nh lá»—i 429 náº¿u cháº¡y quÃ¡ nhanh
                            time.sleep(1.5)
                            
                            count += 1
                            my_bar.progress(count / len(hard_cases))
                        
                        st.success(f"âœ… ÄÃ£ rÃ  soÃ¡t xong {len(hard_cases)} dÃ²ng!")
                        st.rerun() # Load láº¡i trang Ä‘á»ƒ hiá»ƒn thá»‹ káº¿t quáº£ má»›i

        with col_ai_2:
            st.write("### ğŸ“¥ Xuáº¥t Káº¿t Quáº£")
            # Hiá»ƒn thá»‹ dataframe káº¿t quáº£
            st.dataframe(st.session_state.result_df, height=300)
            
            # Logic xuáº¥t Excel
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                st.session_state.result_df.to_excel(writer, index=False, sheet_name='KetQua')
            
            st.download_button(
                label="Táº£i file Excel (.xlsx)",
                data=buffer,
                file_name="Pharma_Map_Result_AI.xlsx",
                mime="application/vnd.ms-excel"
            )

# --- TAB 4: TRAINING MODEL (Tá»ª FILE 01 - GIÃšP MÃY KHÃ”N HÆ N) ---
with tab4:
    st.subheader("4ï¸âƒ£ Huáº¥n luyá»‡n AI (Supervised Learning)")
    st.info("Náº¿u mÃ¡y nháº­n diá»‡n sai hÃ£ng (vÃ­ dá»¥: 'DHG' khÃ´ng ra 'DÆ°á»£c Háº­u Giang'), hÃ£y upload file lá»‹ch sá»­ Ä‘Ã£ map Ä‘Ãºng Ä‘á»ƒ dáº¡y láº¡i mÃ¡y.")
    
    uploaded_hist = st.file_uploader("Chá»n file lá»‹ch sá»­ mapping (.xlsx)", key="hist")
    
    if uploaded_hist:
        df_hist = pd.read_excel(uploaded_hist)
        st.write("Dá»¯ liá»‡u máº«u:")
        st.dataframe(df_hist.head(3))
        
        c1, c2 = st.columns(2)
        col_in = c1.selectbox("Cá»™t TÃªn Gá»‘c (Input) - VÃ­ dá»¥: Ten_Thuoc", df_hist.columns)
        col_out = c2.selectbox("Cá»™t HÃ£ng Chuáº©n (Target) - VÃ­ dá»¥: NSX_Chuan", df_hist.columns)
        
        if st.button("ğŸ“ Báº®T Äáº¦U Dáº Y MÃY"):
            with st.spinner("Äang phÃ¢n tÃ­ch quy luáº­t tá»« ngá»¯..."):
                # Gá»i hÃ m learn tá»« Class PharmaBrain
                n_learned = st.session_state.brain.learn(df_hist, col_in, col_out)
                st.session_state.brain.save_model()
            
            st.success(f"ğŸ‰ ÄÃ£ há»c xong! MÃ¡y Ä‘Ã£ ghi nhá»› thÃªm {n_learned} tá»« khÃ³a nháº­n diá»‡n hÃ£ng má»›i.")
            
            with st.expander("Xem bá»™ nhá»› (Brand Memory)"):
                st.json(st.session_state.brain.brand_memory)
