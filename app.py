import streamlit as st
import pandas as pd
import google.generativeai as genai
from rapidfuzz import fuzz, process
import unidecode
import json
import time
import os
from datetime import datetime

# --- C·∫§U H√åNH ---
st.set_page_config(page_title="D∆∞·ª£c V∆∞∆°ng Speed Map", layout="wide")

# Cache d·ªØ li·ªáu VTMA ƒë·ªÉ kh√¥ng ph·∫£i load l·∫°i m·ªói l·∫ßn click
@st.cache_data
def load_vtma_data():
    try:
        df = pd.read_csv("data/vtma_standard.csv")
        # T·∫°o c·ªôt text t·ªïng h·ª£p ƒë·ªÉ so s√°nh nhanh
        df['search_text'] = df.apply(lambda x: normalize_text(f"{x['ten_thuoc']} {x['hoat_chat']} {x['ten_cong_ty']}"), axis=1)
        return df
    except:
        return pd.DataFrame()

def normalize_text(text):
    if pd.isna(text): return ""
    return unidecode.unidecode(str(text).lower()).strip()

# --- G·ªåI AI (Ch·ªâ d√πng khi c·∫ßn thi·∫øt) ---
def get_ai_info(product_name, api_key):
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        prompt = f"""
        Ph√¢n t√≠ch thu·ªëc: "{product_name}". 
        Tr·∫£ v·ªÅ JSON keys: "active_ingredient", "strength", "brand_name", "manufacturer".
        N·∫øu kh√¥ng r√µ ƒë·ªÉ null.
        """
        response = model.generate_content(prompt)
        text = response.text.replace('```json', '').replace('```', '').strip()
        return json.loads(text)
    except:
        return {}

# --- H√ÄM T√çNH ƒêI·ªÇM (ƒê√£ t·ªëi ∆∞u) ---
def calculate_score_detailed(input_info, db_row, is_ai_data=False):
    score = 0
    
    # N·∫øu d·ªØ li·ªáu t·ª´ AI
    if is_ai_data:
        if input_info.get('active_ingredient'):
            score += fuzz.token_sort_ratio(normalize_text(input_info['active_ingredient']), normalize_text(db_row['hoat_chat'])) * 0.4
        if input_info.get('strength'):
            score += fuzz.ratio(normalize_text(input_info['strength']), normalize_text(db_row['ham_luong'])) * 0.3
        score += fuzz.token_set_ratio(normalize_text(input_info.get('brand_name','')), normalize_text(db_row['ten_thuoc'])) * 0.2
        if input_info.get('manufacturer'):
            score += fuzz.partial_ratio(normalize_text(input_info['manufacturer']), normalize_text(db_row['ten_cong_ty'])) * 0.1
            
    # N·∫øu d·ªØ li·ªáu th√¥ (So s√°nh chu·ªói tr·ª±c ti·∫øp)
    else:
        # So kh·ªõp t√™n thu·ªëc D∆∞·ª£c V∆∞∆°ng v·ªõi (T√™n + Ho·∫°t ch·∫•t VTMA)
        score = fuzz.token_set_ratio(input_info, db_row['search_text'])
        
    return round(score, 1)

# --- GIAO DI·ªÜN ---
st.title("‚ö° PharmaMatch Speed: Map D·ªØ Li·ªáu T·ªëc ƒê·ªô Cao")

with st.sidebar:
    st.header("C√†i ƒë·∫∑t")
    user_api_key = st.text_input("Gemini API Key", type="password")
    if not user_api_key and "GENAI_API_KEY" in st.secrets:
        user_api_key = st.secrets["GENAI_API_KEY"]
        
    threshold = st.slider("Ng∆∞·ª°ng g·ªçi AI (%)", 50, 90, 70, help="N·∫øu so kh·ªõp th√¥ d∆∞·ªõi m·ª©c n√†y m·ªõi g·ªçi AI")
    top_n = st.number_input("S·ªë m√£ g·ª£i √Ω", 1, 5, 1)

vtma_df = load_vtma_data()
if vtma_df.empty:
    st.error("‚ùå Ch∆∞a c√≥ file data/vtma_standard.csv")
    st.stop()
else:
    st.success(f"‚úÖ Database: {len(vtma_df)} m√£")

uploaded = st.file_uploader("Upload File D∆∞·ª£c V∆∞∆°ng", type=['xlsx', 'csv'])

if uploaded and st.button("üöÄ CH·∫†Y MAPPING SI√äU T·ªêC"):
    if not user_api_key:
        st.warning("‚ö†Ô∏è C·∫ßn API Key ƒë·ªÉ x·ª≠ l√Ω c√°c ca kh√≥!")
        st.stop()
        
    if uploaded.name.endswith('.csv'): df_in = pd.read_csv(uploaded)
    else: df_in = pd.read_excel(uploaded)
    
    col_name = df_in.columns[0]
    results = []
    
    # Thanh progress bar
    progress_text = "ƒêang x·ª≠ l√Ω..."
    my_bar = st.progress(0, text=progress_text)
    
    # Chuy·ªÉn c·ªôt VTMA search text sang list ƒë·ªÉ t√¨m ki·∫øm vector nhanh h∆°n
    vtma_search_list = vtma_df['search_text'].tolist()
    vtma_indices = vtma_df.index.tolist()
    
    total_rows = len(df_in)
    ai_call_count = 0
    
    for i, row in df_in.iterrows():
        raw_name = str(row[col_name])
        normalized_name = normalize_text(raw_name)
        
        # B∆Ø·ªöC 1: QU√âT NHANH (FAST SCAN)
        # T√¨m 5 ·ª©ng vi√™n s√°ng gi√° nh·∫•t d·ª±a tr√™n text thu·∫ßn t√∫y
        # process.extract d√πng thu·∫≠t to√°n C++ n√™n c·ª±c nhanh, kh√¥ng c·∫ßn loop th·ªß c√¥ng
        candidates_raw = process.extract(normalized_name, vtma_search_list, limit=10, scorer=fuzz.token_set_ratio)
        
        best_match = None
        best_score = 0
        
        # Ki·ªÉm tra ·ª©ng vi√™n t·ªët nh·∫•t
        if candidates_raw:
            top_candidate_text, top_score, top_index = candidates_raw[0]
            if top_score >= threshold:
                # N·∫øu ƒëi·ªÉm cao -> CH·ªêT LU√îN (Kh√¥ng g·ªçi AI)
                best_match = vtma_df.iloc[top_index]
                best_score = top_score
                method = "Text Match (Nhanh)"
            else:
                # N·∫øu ƒëi·ªÉm th·∫•p -> G·ªåI AI (Ch·∫≠m nh∆∞ng ch·∫Øc)
                ai_data = get_ai_info(raw_name, user_api_key)
                ai_call_count += 1
                method = "AI Analysis (S√¢u)"
                
                # T√≠nh l·∫°i ƒëi·ªÉm v·ªõi th√¥ng tin AI
                re_ranked = []
                # Ch·ªâ so s√°nh l·∫°i v·ªõi 10 ·ª©ng vi√™n ti·ªÅm nƒÉng l√∫c n√£y (ƒë·ª° ph·∫£i qu√©t c·∫£ 10.000 d√≤ng)
                for _, _, idx in candidates_raw:
                    v_row = vtma_df.iloc[idx]
                    s = calculate_score_detailed(ai_data, v_row, is_ai_data=True)
                    re_ranked.append((v_row, s))
                
                # Sort l·∫°i
                re_ranked.sort(key=lambda x: x[1], reverse=True)
                if re_ranked:
                    best_match, best_score = re_ranked[0]

        # Ghi k·∫øt qu·∫£
        if best_match is not None:
             results.append({
                'DV_Input': raw_name,
                'VTMA_Code': best_match['ma_thuoc'],
                'VTMA_Name': best_match['ten_thuoc'],
                'VTMA_HoatChat': best_match['hoat_chat'],
                'Score': best_score,
                'Method': method
            })
        else:
            results.append({'DV_Input': raw_name, 'Status': 'Kh√¥ng t√¨m th·∫•y'})
            
        # Update progress
        my_bar.progress((i + 1) / total_rows, text=f"ƒê√£ x·ª≠ l√Ω {i+1}/{total_rows} (G·ªçi AI: {ai_call_count} l·∫ßn)")

    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    st.success(f"Ho√†n t·∫•t! Ch·ªâ ph·∫£i g·ªçi AI {ai_call_count}/{total_rows} d√≤ng.")
    res_df = pd.DataFrame(results)
    st.dataframe(res_df)
    
    # Download
    os.makedirs('output', exist_ok=True)
    fname = f"output/map_sieutoc_{datetime.now().strftime('%H%M')}.xlsx"
    res_df.to_excel(fname, index=False)
    with open(fname, "rb") as f:
        st.download_button("üì• T·∫£i k·∫øt qu·∫£", f, file_name="ket_qua_sieu_toc.xlsx")
